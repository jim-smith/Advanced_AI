{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 2: Text Pre-processing and Representation\n",
    "\n",
    "In the previous practical we gathered movie reviews from IMDB and annotated them with sentiment. When data is scraped from the web, or even gathered from other sources, it is unlikely to be in a suitable format for NLP applications. So, now that we have some data, the next step is to clean and normalise the text. This will reduce noise within the data and ensure a consistent set of input features for an ML model. Very frequent or infrequent words, punctuation and other characters, emojis, HTML tags etc all increase the number of features present within the data. Each of these may, or may not, be helpful when training a model for a given task.\n",
    "\n",
    "In the first part of this practical we will examine several text pre-processing and normalisation steps and the process of building a vocabulary. Then develop a function to apply each of these steps to our imdb review data.\n",
    "\n",
    "In the second part of this practical we will look at several different methods of representing text in a format that is compatible with ML models, i.e. as numbers or vectors.\n",
    "\n",
    "In the final part you will apply the text pre-processing function and create a vocabulary for a larger dataset, ready for classification next week.\n",
    "\n",
    "The objectives of this practical are:\n",
    "1. Understand various text pre-processing options and determine which are appropriate for a given problem\n",
    "\n",
    "2. Develop text pre-processing and create a vocabulary functions\n",
    "\n",
    "3. Explore vectorised language representations - BOW, One-hot and TF-IDF\n",
    "\n",
    "4. Understand the benefit of word vectors and how to use them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Text Pre-processing\n",
    "\n",
    "## 1.0 Import libraries\n",
    "\n",
    "1. [spaCy](https://spacy.io/) - is a Python library for NLP. It's very efficient and has an excellent set of features.\n",
    "\n",
    "2. [Natural Language Toolkit (NLTK)](https://www.nltk.org/) - is an older but more comprehensive NLP toolkit for Python.\n",
    "\n",
    "3. [Unidecode](https://pypi.org/project/Unidecode/) - is a small Python package for stripping accents from letters.\n",
    "\n",
    "4. [Contractions](https://github.com/kootenpv/contractions) - is a small Python package for expanding contractions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import spacy\n",
    "import unidecode\n",
    "import contractions\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "# Set the directory to the data folder\n",
    "data_dir = os.path.join('..', 'data', 'imdb')\n",
    "\n",
    "# Spacy needs to install the language model also\n",
    "# If you recieve an error, uncomment the following line and re-run the cell\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Pre-processing options\n",
    "\n",
    "The following cells demonstrate each of the pre-processing options discussed in the lecture. For most we use spaCy but several are possible using regular expressions or plain Python.\n",
    "\n",
    "It is very unlikely, you would ever need to apply **all** of these steps. In fact you probably wouldn't have much text left if you did! But it is important to understand what each does and when they might be appropriate.\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Note:</b> The <i>order</i> these steps are applied can sometimes make a big difference.<br>\n",
    "For example, if you were to remove punctuation and replace with an empty string, then hyphenated words would be joined together. So, 'father-in-law' becomes 'fatherinlaw'.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisation and Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy document object\n",
    "raw_text = \"Let's visit my father-in-law in St. Louis next year. He said 'it would be fun!'\"\n",
    "doc = nlp(raw_text)\n",
    "print(\"Document: \" + str(doc))\n",
    "\n",
    "# Segment the text into sentences\n",
    "sentences = list([sent for sent in doc.sents])\n",
    "print(\"Sentences: \" + str(sentences))\n",
    "\n",
    "# Tokenise the sentences\n",
    "tokens = []\n",
    "for sent in doc.sents:\n",
    "    tokens.append([token.text for token in sent])\n",
    "print(\"Tokens: \" + str(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create NLTk stemmer\n",
    "stemmer = SnowballStemmer(language='english')\n",
    "\n",
    "# Create spacy document object\n",
    "raw_text = \"studies studying cries cry automatic automation are is car cars am\"\n",
    "doc = nlp(raw_text)\n",
    "\n",
    "# Print the stem and lemma for each token\n",
    "print(f\"{'Token:':20} {'Stem:':20} {'Lemma:':20}\\n\")\n",
    "for token in doc:\n",
    "    print(f\"{token.text:20} {stemmer.stem(token.text):20} {token.lemma_:20}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop words, Case-folding and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print Spacy's default stop words\n",
    "print(\"List of stop words: \" + str(list(nlp.Defaults.stop_words)[:50]) + \"\\n\")\n",
    "\n",
    "# Create spacy document object\n",
    "raw_text = \"Let's visit my #father-in-law @ St. Louis next year.\\n He said 'it would be fun!'\"\n",
    "doc = nlp(raw_text)\n",
    "print(\"Document: \" + str(doc))\n",
    "\n",
    "# Remove stop words\n",
    "print(\"Removed stop words:\")\n",
    "for sent in doc.sents:\n",
    "    sent = [token for token in sent if not token.is_stop]\n",
    "    print(sent)\n",
    "\n",
    "# Lowercase the tokens\n",
    "# Python: text.lower()\n",
    "print(\"Lower-cased words:\")\n",
    "for sent in doc.sents:\n",
    "    sent = [token.lower_ for token in sent]\n",
    "    print(sent)\n",
    "\n",
    "# Remove punctuation\n",
    "# Regex: keep only letters and numbers\n",
    "# re.sub('[^A-Za-z0-9]+', ' ', text)\n",
    "print(\"Removed punctuation:\")\n",
    "for sent in doc.sents:\n",
    "    sent = [token for token in sent if not token.is_punct]\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whitespace, characters, contractions, accents, HTML tags and emoji\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Note:</b> The regex for removing emojis is from <a href=https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python>this stack overflow answer</a>.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-warning\" role=\"alert\">\n",
    "<b>Parsing HTML with regex:</b> The regular expression used here works reasonably well for simple HTML tags but is not fool proof, as jokingly outlined <a href=https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags?noredirect=1&lq=1>in this well known stack overflow answer</a>.<br>\n",
    "\n",
    "Regex cannot reliably account for the complex structure of HTML, so if it is critical to correctly parse HTML you should use an XML parser or something like Beautiful soup.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy document object\n",
    "raw_text = u\"<a href='site.com' class='link'> Let's visit mý \\t #fáther-in-law @ St. Louis next year.\\n <b>He said 'it would be fun!' \\U0001f602 </b></a><br>\"\n",
    "\n",
    "doc = nlp(raw_text)\n",
    "print(\"Document: \" + str(doc))\n",
    "\n",
    "# Remove whitespace\n",
    "# Regex: remove 1 or more whitespace characters\n",
    "# re.sub('\\s+', ' ', text)\n",
    "print(\"Removed whitespace characters:\")\n",
    "for sent in doc.sents:\n",
    "    sent = [token for token in sent if not token.is_space]\n",
    "    print(sent)\n",
    "\n",
    "# Remove specific characters\n",
    "# Characters are specified inside the square brackets\n",
    "print(\"Removed specific characters:\")\n",
    "for sent in doc.sents:\n",
    "    sent = re.sub('[@#$]', '', sent.text)\n",
    "    print(sent)\n",
    "\n",
    "# Remove accents\n",
    "print(\"Removed accents:\")\n",
    "for sent in doc.sents:\n",
    "    sent = unidecode.unidecode(sent.text)\n",
    "    print(sent)\n",
    "\n",
    "# Expand contractions\n",
    "print(\"Expanded contractions:\")\n",
    "for sent in doc.sents:\n",
    "    sent = contractions.fix(sent.text)\n",
    "    print(sent)\n",
    "\n",
    "# Remove HTML tags\n",
    "# Match 0 or more characters between < and >\n",
    "print(\"Removed HTML tags:\")\n",
    "for sent in doc.sents:\n",
    "    sent = re.sub('<.*?>', '', sent.text)\n",
    "    print(sent)\n",
    "\n",
    "# Remove emojis\n",
    "emoji_pattern = re.compile(\"[\"\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                        \"]+\", flags=re.UNICODE)\n",
    "print(\"Removed emoji:\")\n",
    "for sent in doc.sents:\n",
    "    sent = emoji_pattern.sub(r'', sent.text)\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Building a vocabulary\n",
    "\n",
    "It is often helpful to create a vocabulary once the text has been processed. At a certain point words appear so infrequently they may have little impact on the model. So a vocabulary allows us to choose how many words (features) to keep and then discard those that are less frequently occuring.\n",
    "\n",
    "A vocabulary also alows us to map word tokens to indices to perform simple text **vectorisation**. And also add special tokens such as `<unk>` to replace unknown/out-of-vocabulary (OOV) words, and `<pad>` to pad inputs to a given length.\n",
    "\n",
    "\n",
    "1. First pre-process/normalise the text.\n",
    "\n",
    "2. Then use `Counter()` to create a dictionary of words and frequency counts.\n",
    "\n",
    "3. Finally create a vocabulary (list) and add the `vocab_size` number of most frequently occuring words. Note we also added `<unk>` and `<pad>` at the beginning. We will use these in future weeks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy document object\n",
    "raw_text = \"Let's visit my father-in-law in St. Louis next year.\\n He said 'it would be fun!'.\"\n",
    "doc = nlp(raw_text)\n",
    "print(\"Document: \" + str(doc))\n",
    "\n",
    "# Do some pre-processing\n",
    "# Let's just lowercase the tokens and remove whitespace characters\n",
    "corpus = []\n",
    "for sent in doc.sents:\n",
    "    sent = [token.lower_ for token in sent]\n",
    "    sent = [token.strip() for token in sent]\n",
    "    corpus.append(sent)\n",
    "\n",
    "# Count the frequency of each token in the corpus\n",
    "word_counter = Counter()\n",
    "for sent in corpus:\n",
    "    word_counter.update(sent)\n",
    "print(word_counter)\n",
    "print(\"Total word count: \" + str(len(word_counter)))\n",
    "\n",
    "# Create a vocabulary of vocab_size, also include special tokens\n",
    "vocab_size = 20\n",
    "special_tokens = ['<pad>', '<unk>']\n",
    "vocab = []\n",
    "\n",
    "# Add the special tokens to the vocabulary\n",
    "vocab.extend(special_tokens)\n",
    "\n",
    "# Add the vocab_size most common tokens to the vocabulary\n",
    "vocab.extend([word for word, count in word_counter.most_common(vocab_size - len(special_tokens))])\n",
    "print(vocab)\n",
    "print(\"Vocabulary size: \" + str(len(vocab)))\n",
    "\n",
    "# Now we can get the index for a token, or the token from an index\n",
    "print(vocab.index('father'))\n",
    "print(vocab[vocab.index('father')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Exercise: Pre-processing pipeline\n",
    "\n",
    "We have now seen each various pre-processing options applied individually. However, several of these steps will need to be applied at the same time. The appropriate steps to apply are problem specific and choice of approache is all part of a NLP project development. At the very least you will probably need to remove extra whitespace and tokenise the text, but most likely case-folding and removing some special characters will be necessary too. \n",
    "\n",
    "Libraries like NLTK, spaCy and textaCy can help you build a processing 'pipeline' but it is convenient to create a function or class to handle these steps for you.\n",
    "\n",
    "1. In the following cell complete the `preprocess_text()` function. It should take a single string as input, apply a range of processing options and either return a list of tokens, if `tokenise=True`, or a string.\n",
    "\n",
    "2. The function should apply case-folding, expand contractions, lemmatise, remove punctuation, whitespace, accents, basic HTML tags and emojis. It should also include arguments to select and apply each of these options separately e.g. `to_lower=False`.\n",
    "\n",
    "3. You can use the `test_text` string to develop the function. Remember the *order* you apply different steps can make a big difference!\n",
    "\n",
    "4. Once you are happy, load your IMDB reviews from the .csv file and apply the function to each review. It might take some trial and error to find the right pre-processing options for your reviews.\n",
    "\n",
    "5. Finally, you should convert the processed reviews into a list where each item is a single tokenised review and make sure you name it `imdb_corpus`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to pre-process the corpus\n",
    "def preprocess_text(text, tokenise=False):\n",
    "    # You can add more pre-processing steps here\n",
    "    pass\n",
    "\n",
    "test_text = \"<a href='https://www.imdb.com/title/tt0000417/reviews/?ref_=tt_ql_urv'>I can now say that I've seen a movie that's over 100 years old</a> Georges Méliès's 1902 masterpiece is not just a science fiction movie. <br /><br />It's also a satire on nineteenth-century science.\\t\"\"Le Voyage dans la Lune\"\" (\"\"A Trip to the Moon\"\") is also an indictment of colonialism.\\nThe astronauts attack the Moon Men - called \\\"Selenites\\\" - and then bring one back to Earth, where they parade him around. \"\" \\U0001f602\"\n",
    "print(test_text)\n",
    "\n",
    "text = preprocess_text(test_text,)\n",
    "print(text)\n",
    "\n",
    "# Load the imdb reviews\n",
    "imdb_reviews = pd.read_csv(os.path.join(data_dir, 'imdb_reviews_raw.csv'), index_col=0)\n",
    "\n",
    "# Apply the preprocessing function to each review in the reviews column\n",
    "imdb_corpus = imdb_reviews['review'].apply(lambda x: preprocess_text(x)).tolist()\n",
    "print(imdb_corpus[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Exercise: Create a vocabulary\n",
    "\n",
    "Once the reviews have been pre-processed create a vocabulary from the corpus.\n",
    "\n",
    "1. In the following cell complete the `create_vocabulary()` function. It should take a list of tokenised sentences as input and return a list of words.\n",
    "\n",
    "2. It should also include arguments to set the (maximum) `vocab_size` and include `special_tokens`, e.g. `special_tokens=['<pad>', '<unk>']`.\n",
    "\n",
    "3. Once the function is complete choose an appropriate `vocab_size` (1-2k words), do not include special tokens for now and create and name the vocabulary something unique like `imdb_vocab`.\n",
    "\n",
    "4. Print the most common 50-100 tokens and see if these are what you expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(corpus, vocab_size=None, min_freq=1, special_tokens=None):\n",
    "    # You can add the vocabulary creation code here\n",
    "    pass\n",
    "\n",
    "# Set vocab_size and special tokens\n",
    "vocab_size = 3000\n",
    "special_tokens = None\n",
    "\n",
    "# Create a vocabulary\n",
    "imdb_vocab = create_vocabulary(imdb_corpus)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary size: \" + str(len(imdb_vocab)))\n",
    "for i, word in enumerate(imdb_vocab[:50]):\n",
    "    print(f'({str(i)}, {word})', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Language Representation\n",
    "\n",
    "## 2.0 Import libraries\n",
    "\n",
    "1. [Sklearn (scikit-learn)](https://scikit-learn.org/stable/) - is a comprehensive Python library for Machine Learning. We will use its text pre-processing features and also for PCA.\n",
    "\n",
    "2. [Gensim](https://radimrehurek.com/gensim/index.html) - is primarily a Python Topic Modelling library. It also has lots of useful features for working with Word Vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import ngrams\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Increase pandas display width\n",
    "pd.set_option('display.width', 500)\n",
    "# Set seaborn style\n",
    "sns.set_style(\"whitegrid\", {'axes.grid' : False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Representation options\n",
    "\n",
    "The following cells demonstrate each of the language representation options discussed in the lecture. For most we use numpy/plain Python to demonstrate the process and then sklearn's built in functions.\n",
    "\n",
    "Like pre-processing the approapriate representation is dependant on the task, and generally the input *shape* of the data for a given model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot Encoding\n",
    "\n",
    "One-hot encoding converts a word into an array of length `vocab_size`, with a **1** at the index of the words position in the vocabulary and **0's** in every other position. Encoding a sentence then becomes a 2D array of shape `vocab_size` x `sequence_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy document object\n",
    "text = \"This is a test sentence which is a very long test sentence.\"\n",
    "doc = nlp(text)\n",
    "print(\"Document: \" + str(doc))\n",
    "\n",
    "# Tokenise the document\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens: \" + str(tokens))\n",
    "\n",
    "# Create simple vocabulary\n",
    "vocab = list(set(tokens))\n",
    "print(\"Vocabulary: \" + str(vocab))\n",
    "\n",
    "# Get a list of token indices within vocabulary\n",
    "token_indices = [vocab.index(token) for token in tokens]\n",
    "print(\"Token indices: \" + str(token_indices))\n",
    "\n",
    "# Create a one-hot vector with numpy\n",
    "num_unique = len(vocab) # Need to know how many features there are\n",
    "one_hot = np.eye(num_unique)[token_indices]\n",
    "print(\"One-hot vector with numpy:\")\n",
    "print(one_hot)\n",
    "\n",
    "# Create a one-hot vector with sklearn\n",
    "token_indices = np.array(token_indices).reshape(-1, 1) # Need to reshape the array to 2D\n",
    "one_hot = OneHotEncoder(sparse=False).fit_transform(token_indices)\n",
    "print(\"One-hot vector with sklearn:\")\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag-of-words (BOW)\n",
    "\n",
    "BOW converts a sentence into an array of length `vocab_size`. Simply count the number of times a word appears within the sequence and increment the index according to its position within the vocabulary.\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Note:</b> The output of sklearns CountVectorizer() is different to the numpy implementation. Can you work out why?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy document object\n",
    "text = \"This is a test sentence which is a very long test sentence.\"\n",
    "doc = nlp(text)\n",
    "print(\"Document: \" + str(doc))\n",
    "\n",
    "# Tokenise the document\n",
    "tokens = [token.text for token in doc]\n",
    "print(\"Tokens: \" + str(tokens))\n",
    "\n",
    "# Create simple vocabulary (add word not in our input text)\n",
    "vocab = list(set(tokens + ['supercalifragilisticexpialidocious']))\n",
    "print(\"Vocabulary: \" + str(vocab))\n",
    "\n",
    "# Get a list of token indices within vocabulary\n",
    "token_indices = [vocab.index(token) for token in tokens]\n",
    "print(\"Token indices: \" + str(token_indices))\n",
    "\n",
    "# Create a BOW with numpy\n",
    "bow = np.zeros(len(vocab), dtype=np.int32)\n",
    "for i in range(len(token_indices)):\n",
    "    bow[token_indices[i]] += 1\n",
    "print(\"BOW with numpy:\")\n",
    "print(bow)\n",
    "\n",
    "# Create a BOW with sklearn\n",
    "bow_vectoriser = CountVectorizer(vocabulary=vocab, lowercase=False)\n",
    "bow = bow_vectoriser.fit_transform([text])\n",
    "print(\"BOW with sklearn:\")\n",
    "print(bow.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF\n",
    "\n",
    "TF-IDF converts a corpus into an array of length `num_documents` x `vocab_size`. TF is the frequency of a word within a *document* and IDF is frequency of a word within the *corpus*. The TF-IDF for a word is then TF(word) x IDF(word):\n",
    "\n",
    "$w =$ word/term\n",
    "\n",
    "$d =$ document\n",
    "\n",
    "$N =$ number of documents in corpus\n",
    "\n",
    "$TF(w) = \\frac{count(w, d)}{len(d)}$\n",
    "\n",
    "$IDF(w) = log\\frac{N}{\\sum_{d=1}^{N} count(w, d) + 1}$\n",
    "\n",
    "$TF-IDF(w) = TF(w) \\times IDF(w)$\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Note:</b> The class TF_IDF() mimics the sklearn implementation (as best as possible). Try different normalisations ('l1' or 'l2') and set smoothing True/False.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFIDF():\n",
    "    \n",
    "    def __init__(self, tokeniser=None, vocabulary=None, norm=None, smooth_idf=True):\n",
    "        \"\"\" Arguments:\n",
    "                tokeniser: A function that takes a string and returns a list of tokens.\n",
    "                vocabulary: A list of tokens to use as the vocabulary.\n",
    "                norm: The normalisation to use when calculating the tf-idf vectors.\n",
    "                smooth_idf: Whether to use Laplace smoothing when calculating the idf.\n",
    "        \"\"\"\n",
    "        self.corpus = None\n",
    "        self.N = None\n",
    "        self.tokeniser = tokeniser\n",
    "        self.vocabulary = vocabulary\n",
    "        self.norm = norm\n",
    "        self.smooth_idf = smooth_idf\n",
    "\n",
    "        if not self.tokeniser:\n",
    "            self.tokeniser = self._tokenise\n",
    "\n",
    "        # l1 norm is the sum of the absolute values of the vector\n",
    "        if self.norm and self.norm == 'l1':\n",
    "            self.norm = 1\n",
    "        # l2 norm is the square root of the sum of the squared values of the vector\n",
    "        elif self.norm and self.norm == 'l2':\n",
    "            self.norm = 2\n",
    "\n",
    "    def _tokenise(self, s):\n",
    "        return s.split()\n",
    "\n",
    "    def get_vocabulary(self):\n",
    "        vocab = []\n",
    "        for doc in self.corpus:\n",
    "            vocab.extend(self.tokeniser(doc))\n",
    "\n",
    "        vocab = list(set(vocab))\n",
    "        vocab.sort()\n",
    "        return vocab\n",
    "\n",
    "    def _tf(self):\n",
    "        \"\"\"Get the term frequency for each document in the corpus.\"\"\"\n",
    "\n",
    "        tf = []\n",
    "        for doc in self.corpus:\n",
    "            tf.append(Counter(self.tokeniser(doc)))\n",
    "        return tf\n",
    "\n",
    "    def _df(self):\n",
    "        \"\"\"Get the document frequency of each word in the corpus.\"\"\"\n",
    "\n",
    "        df = Counter()\n",
    "        for doc in self.corpus:\n",
    "            df.update(set(self.tokeniser(doc)))\n",
    "        return df\n",
    "\n",
    "    def _idf(self):\n",
    "        \"\"\"Calculate inverse document frequency for each word in the vocabulary.\"\"\"\n",
    "\n",
    "        # Calculate the DF\n",
    "        df = self._df()\n",
    "\n",
    "        idf = {}\n",
    "        for word in self.vocabulary:\n",
    "            if self.smooth_idf:\n",
    "                idf[word] = 1.0 + np.log((self.N + 1) / (df[word] + 1))\n",
    "            else:\n",
    "                idf[word] = 1.0 + np.log(np.divide(self.N, df[word]))\n",
    "        return idf\n",
    "\n",
    "    def _tfidf(self):\n",
    "        \"\"\"Calculate the TF-IDF for each document in the corpus.\"\"\"\n",
    "\n",
    "        # Calculate TF and IDF\n",
    "        tf = self._tf()\n",
    "        idf = self._idf()\n",
    "\n",
    "        # Calculate TF-IDF\n",
    "        tfidf = np.zeros((self.N, len(self.vocabulary)))\n",
    "\n",
    "        for i, doc in enumerate(self.corpus):\n",
    "            for j, word in enumerate(self.vocabulary):\n",
    "                tfidf[i, j] = tf[i][word] * idf[word]\n",
    "        \n",
    "        if self.norm:\n",
    "            tfidf = tfidf / np.linalg.norm(tfidf, ord=self.norm, axis=1, keepdims=True)\n",
    "        return tfidf\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        # Set corpus/N\n",
    "        self.corpus = np.array(corpus)\n",
    "        self.N = len(self.corpus)\n",
    "\n",
    "        # Set vocabulary\n",
    "        if not self.vocabulary:\n",
    "            self.vocabulary = self.get_vocabulary()\n",
    "\n",
    "        # Calculate TF-IDF\n",
    "        self.tfidf = self._tfidf()\n",
    "        return self\n",
    "\n",
    "    def transform(self, corpus):\n",
    "        # Update corpus/N\n",
    "        self.corpus = np.append(self.corpus, corpus, axis=0)\n",
    "        self.N = len(self.corpus)\n",
    "\n",
    "        # Calculate TF-IDF\n",
    "        self.tfidf = self._tfidf()\n",
    "        return self.tfidf[-len(corpus):]\n",
    "\n",
    "corpus = ['the car is driven on the road', 'the truck is driven on the highway']\n",
    "\n",
    "# Create a TF-IDF with numpy\n",
    "tfidf_numpy = TFIDF(norm='l1', smooth_idf=False).fit(corpus)\n",
    "terms = tfidf_numpy.get_vocabulary()\n",
    "matrix = tfidf_numpy.transform(corpus)\n",
    "print(\"TF-IDF with numpy:\")\n",
    "print(pd.DataFrame(data=matrix, columns=terms))\n",
    "\n",
    "# Transform a new sentence\n",
    "matrix = tfidf_numpy.transform(['the car is driven in the sky'])\n",
    "print(pd.DataFrame(data=matrix, columns=terms))\n",
    "\n",
    "# Create a TF-IDF with sklearn\n",
    "tfidf_sklearn = TfidfVectorizer(norm='l1', smooth_idf=False).fit(corpus)\n",
    "terms_2 = tfidf_sklearn.get_feature_names_out()\n",
    "matrix_2 = tfidf_sklearn.transform(corpus).toarray()\n",
    "print(\"TF-IDF with sklearn:\")\n",
    "print(pd.DataFrame(data=matrix_2, columns=terms_2))\n",
    "\n",
    "# Transform a new sentence\n",
    "matrix_2 = tfidf_sklearn.transform(['the car is driven in the sky']).toarray()\n",
    "print(pd.DataFrame(data=matrix_2, columns=terms_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-grams\n",
    "\n",
    "N-grams are sequences of N words. Typically uni-grams (1), bi-grams (2) and tri-grams (3). Bi-grams and tri-grams (or larger) provide some context to words and can be used as replacement for uni-grams in many models. Here we use NLTK to create tuples of all bi-grams and tri-grams from the text.\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Note:</b> The sklearn CountVectorizer() and TfidfVectorizer() have an <code>ngram_range</code> argument which allows you to vectorise N-grams instead of single words.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spacy document object\n",
    "text = 'I sat by the river bank. I went to the bank to withdraw money.'\n",
    "doc = nlp(text)\n",
    "print(\"Document: \" + str(doc))\n",
    "\n",
    "# Create N-grams with nltk\n",
    "for sent in doc.sents:\n",
    "    print(\"Sentence: \" + str(sent))\n",
    "\n",
    "    tokens = [token.text for token in sent]\n",
    "\n",
    "    bi_grams = list(ngrams(tokens, 2))\n",
    "    print(\"Bi-grams: \" + str(bi_grams))\n",
    "\n",
    "    tri_grams = list(ngrams(tokens, 3))\n",
    "    print(\"Tri-grams: \" + str(tri_grams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Vectors\n",
    "\n",
    "Word vectors represent single words as a vector (list) of real numbers which capture some aspect of their meaning and relationships to other words. The best known (and first) method is [Word2Vec](https://arxiv.org/pdf/1301.3781.pdf) which uses either skip-gram (given context word predict surrounding target words), or a continuous bag of words (predict target word given context words). Word vector models are typically trained on 100's of millions of words to produce a set of weights - an embedding matrix - of shape `vocab_size` x `embedding_dim`, where the embedding dimension is the length of a vector for each word (usually 50 to 300).\n",
    "\n",
    "Once trained these embeddings can be used as semantically rich word representations for other NLP tasks, such as classification. This is called transfer learning, where the weights for a model trained on one objective (predicting words) can be used as input to train models on a different task (classification, language modelling, etc). There are lots of pre-trained word vectors available to download which can be used to map words to vectors for input into your models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Gensim to create a Word2Vec model from our IMDB data.\n",
    "\n",
    "- `sentences` = the input list of lists of tokens.\n",
    "\n",
    "- `size` = dimensionality of the word vectors.\n",
    "\n",
    "- `window` = maximum distance between the current and predicted word within a sentence - the size of the 'sliding window' during training.\n",
    "\n",
    "- `min_count` = ignore words with total frequency lower than this.\n",
    "\n",
    "- `sg` = training algorithm: 1 for skip-gram; otherwise CBOW.\n",
    "\n",
    "- `workers` = number of worker threads to train the model (faster training with multicore machines).\n",
    "\n",
    "Initially the model is untrained, so we are using the existing Word2Vec values for each word. We can view the words vector and the N most similar words (calculated with cosine similarity). If you pick a word that is quite unique to our IMDB data (like 'georges') it is likely that the most similar words don't make much sense. If you instead choose a more common word (like 'film') you should see similar words like cinema.\n",
    "\n",
    "Once the model is 'fine-tuned' on the IMDB data, the corpus-specific words should have more sensible similar words, e.g. 'melies', 'directed' and 'director', for 'georges'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word2vec model with gensim\n",
    "embedding_dim = 100\n",
    "w2v_model = Word2Vec(sentences=imdb_corpus, size=embedding_dim, window=5, min_count=2, sg=1, seed=1, workers=4)\n",
    "print(\"W2v model vocabulary: \" + str(list(w2v_model.wv.vocab)[:100]))\n",
    "\n",
    "N = 10\n",
    "word = 'georges'\n",
    "print(f\"Vector for '{word}':\")\n",
    "print(w2v_model.wv.get_vector(word))\n",
    "print(f\"{N} most similar words to '{word}':\")\n",
    "print(w2v_model.wv.most_similar(word, topn=N))\n",
    "\n",
    "# Train the model for a few epochs\n",
    "w2v_model.train(imdb_corpus, total_examples=len(imdb_corpus), epochs=2)\n",
    "print(f\"Vector for '{word}':\")\n",
    "print(w2v_model.wv.get_vector(word))\n",
    "print(f\"{N} most similar words to '{word}':\")\n",
    "print(w2v_model.wv.most_similar(word, topn=N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have fine-tuned the model we can create an embedding matrix of shape `vocab_size` x `embedding_dim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty numpy array\n",
    "embedding_matrix = np.zeros((len(imdb_vocab), embedding_dim))\n",
    "\n",
    "# For each word in the imdb vocabulary\n",
    "for i, word in enumerate(imdb_vocab):\n",
    "    # If the word is in the word2vec model\n",
    "    if word in w2v_model.wv.vocab:\n",
    "        # Get the vector for the word\n",
    "        embedding_matrix[i] = w2v_model.wv.get_vector(word)\n",
    "    else:\n",
    "        # Get a random vector\n",
    "        embedding_matrix[i] = np.random.uniform(np.min(embedding_matrix), np.max(embedding_matrix), embedding_dim)\n",
    "\n",
    "# Create dataframe with words and vectors\n",
    "embedding_df = pd.DataFrame(embedding_matrix, index=imdb_vocab)\n",
    "embedding_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can calculate the similarity between all words in the embedding matrix.\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Note:</b> We use cosine similarity which is a measure of similarity between two sequences of numbers. It produces a similarity score in the range [0, 1].\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the cosine similarity between the words\n",
    "similarity_matrix = cosine_similarity(embedding_df)\n",
    "# Create dataframe with words and similarity\n",
    "similarity_df = pd.DataFrame(similarity_matrix, columns=imdb_vocab)\n",
    "# Add word as second index\n",
    "similarity_df.insert(0, 'word_ind', imdb_vocab)\n",
    "similarity_df.set_index('word_ind', inplace=True, append=True)\n",
    "similarity_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the vectors to visualise the most similar and least similar words to a given target word.\n",
    "\n",
    "1. We will use Principal Component Analysis (PCA) to reduce the dimensionality of the embeddings so we can visualise them.\n",
    "\n",
    "2. Next find the N most similar and disimilar words to a target word.\n",
    "\n",
    "3. Create a 3D plot of the embeddings. With `N=10` and 'georges' you should see that, for example, 'melies' and 'director' are very close in embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of similar/disimilar words and a target word\n",
    "N = 10\n",
    "word = 'georges'\n",
    "\n",
    "# Perform PCA (dimensionality reduction) on the embedding matrix\n",
    "pca_embeddings = PCA(n_components=3).fit_transform(embedding_matrix)\n",
    "\n",
    "# Find the N most/least similar words\n",
    "most_sim = similarity_df[word].sort_values(ascending=False)[0:N + 1]\n",
    "least_sim = similarity_df[word].sort_values(ascending=True)[0:N + 1]\n",
    "\n",
    "most_sim_words = [w for ind, w in most_sim.index.values]\n",
    "least_sim_words = [w for ind, w in least_sim.index.values]\n",
    "\n",
    "# Get the indices of the most/least similar words from the reduced embedding matrix\n",
    "most_sim_pca = pca_embeddings[[ind for ind, w in most_sim.index.values]]\n",
    "least_sim_pca = pca_embeddings[[ind for ind, w in least_sim.index.values]]\n",
    "\n",
    "# Plot the most/least similar words\n",
    "fig = plt.figure(figsize=(11, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(most_sim_pca[:, 0], most_sim_pca[:, 1],  most_sim_pca[:, 2], linewidths=1, color='blue')\n",
    "ax.scatter(least_sim_pca[:, 0], least_sim_pca[:, 1],  least_sim_pca[:, 2], linewidths=1, color='red')\n",
    "# Add words to the plot\n",
    "for i, word in enumerate(most_sim_words):\n",
    "    ax.text(most_sim_pca[i, 0]+.02, most_sim_pca[i, 1], most_sim_pca[i, 2], word, size=10, zorder=1)\n",
    "for i, word in enumerate(least_sim_words):\n",
    "    ax.text(least_sim_pca[i, 0]+.02, least_sim_pca[i, 1], least_sim_pca[i, 2], word, size=10, zorder=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Processing a Dataset\n",
    "\n",
    "So far we have explored several text pre-processing and representation methods using the IMDB movie reviews that we gathered. However, deep learning requires a lot of data, so we do not have enough to adequately train most models. For this we will use an existing movie review dataset for training and keep ours as an additional test set. The IMDB movie review dataset contains 50,000 reviews and sentiment labels 'positive' and 'negative'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Exercise: Prepare the IMDB dataset\n",
    "\n",
    "You have already developed a `preprocess_text()` function which applies case-folding, expand contractions, lemmatise, remove punctuation, whitespace, accents, basic HTML tags and emojis. So now you can continue to develop its functionality and then apply it to your IMDB reviews and the larger IMDB dataset.\n",
    "\n",
    "1. Extend the function to provide the option to remove only specific characters, rather than all punctuation. For example, you may want to remove brackets and other special characters, but keep full stops, commas etc. Similarly, you could include the option to remove only specific stop words, rather than all of the stop words included with spaCy.\n",
    "\n",
    "2. Load the your IMDB reveiw dataset and apply the pre-processing to each review. You **should not tokenise the data** at this stage. Leave each review as a whole string.\n",
    "\n",
    "3. Save the processed dataset as `imdb_reviews.csv`.\n",
    "\n",
    "4. Once you are happy with the results apply the pre-processing to the larger dataset `imdb_reviews_raw.csv`, and save it as `imdb_reviews.csv`.\n",
    "\n",
    "<div class = \"alert alert-block alert-warning\"><b>Warning:</b> Processing all 50,000 reviews might take some time. You should be sure of the pre-processing options you have selected before you apply them to the entire datset.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imdb reviews\n",
    "imdb_data = pd.read_csv(os.path.join(data_dir, 'imdb_reviews_raw.csv'), index_col=0)\n",
    "\n",
    "# Apply the preprocessing function\n",
    "imdb_data['review'] = imdb_data['review'].apply(lambda x: preprocess_text(x, tokenise=False))\n",
    "\n",
    "# Save the data\n",
    "imdb_data.to_csv(os.path.join(data_dir, 'imdb_reviews.csv'))\n",
    "\n",
    "# Load the full imdb dataset\n",
    "# imdb_data = pd.read_csv(os.path.join(data_dir, 'imdb_dataset_raw.csv'))\n",
    "\n",
    "# # Apply the preprocessing function\n",
    "# imdb_data['review'] = imdb_data['review'].apply(lambda x: preprocess_text(x, tokenise=False))\n",
    "\n",
    "# # Save the data\n",
    "# imdb_data.to_csv(os.path.join(data_dir, 'imdb_dataset.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Exercise: Create an IMDB vocabulary\n",
    "\n",
    "You have already developed a `create_vocabulary()` function, so now you can continue to develop its functionality and then create a vocabulary from the larger IMDB dataset.\n",
    "\n",
    "1. Extend the function to discard words below a certain frequency e.g. `min_freq=2` only includes words that occur 2 times or more (possibly at the expense of the vocabulary size).\n",
    "\n",
    "2. Load your newly processed IMDB dataset and tokenise it. Hint: you can use `.apply(lambda x: [token.text for token in nlp.tokenizer(x)])' to quickly tokenise the strings.\n",
    "\n",
    "3. Set a vocabulry size and then create a vocabulary. You **do not need to add special tokens** for now.\n",
    "\n",
    "4. Save the vocabulary as a text file for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imdb dataset\n",
    "imdb_data = pd.read_csv(os.path.join(data_dir, 'imdb_dataset.csv'))\n",
    "\n",
    "# Tokenise the reviews\n",
    "tokens = imdb_data['review'].apply(lambda x: [token.text for token in nlp.tokenizer(x)])\n",
    "\n",
    "# Set vocab_size\n",
    "vocab_size = 2000\n",
    "\n",
    "# Create a vocabulary\n",
    "imdb_vocab = create_vocabulary(tokens, vocab_size=vocab_size, min_freq=2)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary size: \" + str(len(imdb_vocab)))\n",
    "for i, word in enumerate(imdb_vocab[:50]):\n",
    "    print(f'({str(i)}, {word})', end=' ')\n",
    "\n",
    "# Save to text file\n",
    "with open(os.path.join(data_dir, 'imdb_vocab.txt'), 'w+') as file:\n",
    "    for word in imdb_vocab:\n",
    "        file.write(word + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "eda59365f9d652723e3bcf67739b9100ac1f6ab6ddfa121c8653940903b971a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
