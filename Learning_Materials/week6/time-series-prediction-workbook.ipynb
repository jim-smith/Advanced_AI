{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee97b0-1bd4-4f91-937d-990bba44a5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as Keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059d353a-3416-4958-bca3-569c984465cd",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introducing the keras [timeseries data set from array function](https://keras.io/api/preprocessing/timeseries)\n",
    "\n",
    "## A simple example that fits on-screen\n",
    "To illustrate this lets make a simple data set with four features and 10 time steps.\n",
    "We'll assume:\n",
    "- the first feature is 1,2,...10, \n",
    "- the second is 10,20,... and so on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6605f11-02a3-4e50-b78e-e52703f3b0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = np.zeros([10,4],dtype='int32')\n",
    "for row in range(10):\n",
    "    dataset[row][0] = row\n",
    "    dataset[row][1] = row*10\n",
    "    dataset[row][2] = row*100\n",
    "    dataset[row][3] = row*1000\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b8a57d-39c6-49fa-9b08-478988680185",
   "metadata": {},
   "source": [
    "Now to make it easy to visualise on screen lets say:\n",
    "- we are using a window of size 3,\n",
    "- we are trying to predict the first column, one day in advance\n",
    "- i.e. at time _t_ we give the model x[t], x[t-1], and x[t-2] andf we want to predict x[t+1][0] \n",
    "\n",
    "and use a Keras built in function to give us a tensorflow dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605adfd-73ba-4309-ba9a-1f6f2f56a09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "timeseries = Keras.utils.timeseries_dataset_from_array(\n",
    "    dataset,\n",
    "    sequence_length=window_size, # lets take sequences of length 3\n",
    "    targets=dataset[window_size:,0],# we want to predict feature[0] on the next input after our sequence\n",
    "    sequence_stride=1,\n",
    "    sampling_rate=1,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    start_index=None,\n",
    "    end_index=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1912c20-3c0f-42d7-b04b-876c4884a376",
   "metadata": {},
   "source": [
    "## What's in a dataset?\n",
    "Lots of stuff and lots of functionality!\n",
    "\n",
    "Datasets are designed to be used as part of a production pipeline.  \n",
    "So instead of getting access via indexes (lie you would for pandas or numpy), they\n",
    "provide access via iterators  such as _batch()_ or _take()_\n",
    "\n",
    "For now, it's easiest to look at what this dataset object contains if we convert it to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbefa2d-f9d7-4e6b-b019-d1ce98172a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bc968-5e85-49f6-816a-1fb6df679ac8",
   "metadata": {},
   "source": [
    "### Yuk!\n",
    "We can just about see, if we squint a lot, that this contains 7 pairs of items.\n",
    "- there are 7 because that is how many length 3 sequences you can get from 10 items\n",
    "\n",
    "Each item  has:\n",
    "- a tensor of shape (batchsize, sequence length, num_features)\n",
    "- a scalar value (again wrapped up inside a tensor) \n",
    "  - because we only asked to predict 1 feature (the one at index 0)\n",
    "  \n",
    "\n",
    "### For you to experiment\n",
    "Try  asking for different length sequences (line 1) or different size batches (line 8), or for more than one feature as a label(line 5) and re-running the two code cells above and make sure you understand what you are getting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10afde6-7c29-48c7-8c37-73ee9ba0acef",
   "metadata": {},
   "source": [
    "## But a tensorflow dataset does have some advantages\n",
    "- for example we can ask it batch up the data\n",
    "- and if we pass it to tensorflow preprocessing layers or a model's fit() method they will do that\n",
    "\n",
    "So let's ask our timeseries to give us a load of batches\n",
    "\n",
    "Note that batch() or take() give us the outputs of type batchdataset so we have to iterate over their contents using\n",
    "````\n",
    "    for item in timeseries.batch(batch_size=1):\n",
    "    ````\n",
    "    \n",
    "instead of using slices like we would for numpy arrays or pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7280ca-9903-4bf4-8b3d-af0e4fd40ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('First batches of size 1')\n",
    "for item in timeseries.batch(batch_size=1):\n",
    "    print(f'{item[:-1]} : {item[-1]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a0502-0488-453b-9fea-be423ccfbaa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Now batches of size 2')\n",
    "x2= timeseries.batch(batch_size=2)\n",
    "for item in x2:\n",
    "    print(f'x= {item[:-1]} \\n y= {item[-1]}')\n",
    "print('The last batch only has one thing in, because there are only 7 sequences of length 3 in 0...9') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8816645b-4019-43bd-a5b5-e64ce75cabaf",
   "metadata": {},
   "source": [
    "## Finally, this is one way to convert one of these items back to numpy using a lambda function\n",
    "\n",
    "Lets take the first batch as an example and turn it into a 'windowed' row of size (sequence length *number of features). and a scalar (the label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154771e5-7f4a-4494-8700-a9020cc389f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sequence_to_flat(item):\n",
    "    # get size of array to hold one windowed row\n",
    "    item_shape= item[0].shape\n",
    "    num_windowed_features= item_shape[-2]*item_shape[-1]\n",
    "    X= tf.reshape(item[0],num_windowed_features)\n",
    "    y = tf.reshape(item[1],1)\n",
    "    return(X,y)\n",
    "\n",
    "my_iterator = iter(timeseries)\n",
    "first_item = my_iterator.get_next()\n",
    "X,y = sequence_to_flat(first_item) \n",
    "print(f'when flattened the first item is:\\n {X} : {y}')\n",
    "X,y = sequence_to_flat(my_iterator.get_next())\n",
    "print(f'when flattened the next item is;\\n {X} : {y}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7876f-2499-4e24-81d4-31fbb5f56ae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Comparing three different neural architectures for a time-series prediction problem\n",
    "\n",
    "## Data set description and characteristics\n",
    "Delhi data from [here](https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data)\n",
    "\n",
    "4 variables: temp, humidity, windspeed, pressure\n",
    "Training set: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73949a51-d9b8-4f8f-b3be-2d99ae6b61f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if (socket.gethostname()=='csctcloud'): #on csctcloud\n",
    "    datapath=\"/home/common/datasets\"\n",
    "elif (socket.gethostname()[0:7]=='jupyter'): #on csctcloud\n",
    "    datapath=\"~/shared/datasets\"\n",
    "else: #machine specific- this is for jim's development\n",
    "    datapath = \"../datasets\"\n",
    "reldirname= datapath +\"/delhi/\"\n",
    "train_raw = pd.read_csv(reldirname +\"DailyDelhiClimateTrain.csv\")\n",
    "test_raw =  pd.read_csv(reldirname +\"DailyDelhiClimateTest.csv\")\n",
    "\n",
    "print (f'training data set has {train_raw.shape[0]} '\n",
    "       f'rows and {train_raw.shape[1]} features\\n'\n",
    "       f' it has {train_raw.isna().sum().sum()} nulls\\n'\n",
    "      f'    text data set has  {test_raw.shape[0]} '\n",
    "       f'rows and {test_raw.shape[1]} features\\n'\n",
    "       f' it has {train_raw.isna().sum().sum()} nulls\\n'\n",
    "      )\n",
    "print(f'column names are {train_raw.columns}')\n",
    "train_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9128ed-bbd9-48e5-ad6b-d4c38dd3ecc2",
   "metadata": {},
   "source": [
    "## Fixing the outliers\n",
    "What stands out immediately is that there are some wierd outliers in the meanPressure column.\n",
    "If it turns out that these are isolated odd readings we will replace them by the 50th centile value (~the mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc55e3-b0be-4a09-ab62-a917df35c0f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "median = train_raw['meanpressure'].median()\n",
    "outliers=abs(train_raw['meanpressure'] - 1000) >200\n",
    "train_raw['meanpressure'][outliers] = np.nan\n",
    "train_raw['meanpressure'].fillna(median, inplace=True)\n",
    "train_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbdc3a-654a-433d-938e-6864c2d4b6bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## For now just predict the next day's temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ecd2d-3190-47e7-b008-336ad3012a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_raw.head())\n",
    "\n",
    "#select the mean temp to be the y value\n",
    "#and copy with the date value\n",
    "train_y = train_raw[['date','meantemp']]\n",
    "train_y.set_index('date', inplace=True)\n",
    "#shift works nicely if the index is a datetime object\n",
    "train_y =train_y.shift(periods=1)\n",
    "#fill NaN in first rowwith something sensible\n",
    "train_y.iloc[0] = train_raw['meantemp'].mean()\n",
    "print(train_y.head())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48647e39-bf69-4c12-a2bc-79673ba37b16",
   "metadata": {},
   "source": [
    "## Getting the data ready for ML with Keras \n",
    "\n",
    "Start by making a time series dataset from the train and test data we loaded\n",
    "with the aim of predicting the next day's temperature.   \n",
    "- for simplicity we'll drop the date columns and change everything to numpy arrays\n",
    "- we will leave out all the parameters where the default settings are fine\n",
    "- we'll take a window size of 7 days in case there are weekly effects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3352f1-a203-4210-8d3d-65a47330a53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'date' in train_raw.columns:\n",
    "    train_raw=train_raw.drop(columns=['date'])\n",
    "if 'date' in test_raw.columns:\n",
    "    test_raw=test_raw.drop(columns=['date'])\n",
    "train=train_raw.to_numpy()\n",
    "test = test_raw.to_numpy()\n",
    "\n",
    "print(f'train and test shape {train.shape} , {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6a48a-cf0b-4ab4-ba0d-fcccc7adf8ab",
   "metadata": {},
   "source": [
    "### next we'll apply a standard scale to transform values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6fe57-e3a3-447d-9eab-869f7b9a9d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train=scaler.fit_transform(train)\n",
    "test=scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7fa0e5-07d6-4678-9ccb-b8177df8bd5d",
   "metadata": {},
   "source": [
    "## Some common values to use across all experiments\n",
    "- some e.g. epochs might want tuning - or adapting if you put in early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c32461-0216-4558-8633-1c82fc89435d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_size=7\n",
    "epochs=15\n",
    "larger_batch_size=32\n",
    "num_nodes=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12598fe6-ad6d-4958-bb85-b798f0ac8062",
   "metadata": {},
   "source": [
    "## Finally lets make the basic timeseries datasets\n",
    "and afterwards print out the shape of each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a909af4-a901-4fe2-9f0e-2847987268f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_series = Keras.utils.timeseries_dataset_from_array(\n",
    "    train,\n",
    "    sequence_length=window_size, \n",
    "    targets=train[window_size:,0],\n",
    "    batch_size=1\n",
    "    )\n",
    "\n",
    "test_series = Keras.utils.timeseries_dataset_from_array(\n",
    "    test,\n",
    "    sequence_length=window_size, \n",
    "    targets=test[window_size:,0],\n",
    "    batch_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5499d20-11fa-4eb8-aa91-5ea04e669e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iterator= iter(test_series)\n",
    "print(f'shape of batches from basic datasets is {list( iterator.get_next())[0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c3da7-d933-4e7e-b017-24c7fbd93638",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Algorithm 1: a MLP with a time window.\n",
    "\n",
    "## Preprocess data\n",
    "For this case we can 'flatten' each X item the timeseries dataset from a windowsizex4 array into a 12x1\n",
    "\n",
    "- I've been a bit lazy and worked out the flat size in advance\n",
    "- and i've just printed out the first item from the dataset   \n",
    "  to illustrate the dataset.take() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7aa6ca-fd4d-4dd1-bf42-6c161ccc5e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flatsize = window_size* train.shape[1] \n",
    "\n",
    "def sequence_to_flat2(X,y):\n",
    "    return(tf.reshape(X,[1,flatsize]),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627d10c-9c58-48e9-b794-b2bec400c66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flattened_train = train_series.map(sequence_to_flat2)\n",
    "flattened_test = test_series.map(sequence_to_flat2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eaa75f3-86dd-4a38-bf9b-f04bf45559a2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_item= flattened_train.take(1)\n",
    "print( f'shape of batches in flattened version is now {list(first_item)[0][0].shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d05e7b-e3ad-4215-8710-08cc5433d5bf",
   "metadata": {},
   "source": [
    "### Now lets build a sequential model for our MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15469fa5-0144-4c82-b58f-4c5fbf966272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mlp= Keras.Sequential([\n",
    "    Keras.layers.Dense(num_nodes,activation='relu'),\n",
    "    Keras.layers.Dense(1,activation='linear')]\n",
    "    )\n",
    "mlp.compile(optimizer='adam', loss='mse')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f813133a-f68d-40ea-a6a4-1d76c20755f7",
   "metadata": {},
   "source": [
    "### Note that we can dynamically change the batch size of the datset\n",
    "- from 1: which we used for ease of visualisation\n",
    "- to 32 for speed when fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8f4bf-6787-456a-b5d9-91a77ecfdad0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flattened_train32 = flattened_train.batch(batch_size=larger_batch_size)\n",
    "flattened_test32= flattened_test.batch(batch_size=larger_batch_size)\n",
    "history = mlp.fit(flattened_train32,epochs=epochs,batch_size=larger_batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5953a0f-d68f-411e-bc5b-9ad17c68b5c8",
   "metadata": {},
   "source": [
    "### Let's see how well it did on the training and test data\n",
    "using a neat bit of code from [stackoverflow](https://stackoverflow.com/questions/56226621/how-to-extract-data-labels-back-from-tensorflow-dataset) to get the y labels back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1c2e0-9e6c-4232-a6d8-193341352bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_and_report(model, name,train_ds,test_ds):\n",
    "    ''' gets the train and test mse error\n",
    "        for a given model and train/test datasets\n",
    "        and make a nice plot\n",
    "        Parameters:\n",
    "        ==========\n",
    "        model: trained instance of Keras Sequential or Model class\n",
    "        name: string to use for reporting\n",
    "        train_ds: ndarray or Keras dataset\n",
    "        test_ds: ndarray or keras dataset\n",
    "        '''\n",
    "    trainres=f'Training MSE= {model.evaluate(train_ds)}'\n",
    "    testres=f'Test MSe= {model.evaluate(test_ds)}'\n",
    "\n",
    "    y_train = np.concatenate([y for x, y in train_ds], axis=0)\n",
    "    y_train_pred= model.predict(train_ds).reshape(y_train.shape[0])\n",
    "\n",
    "    y_test = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "    y_test_pred= model.predict(test_ds).reshape(y_test.shape[0])\n",
    "\n",
    "    actual = np.concatenate((y_train,y_test))\n",
    "    predicted= np.concatenate((y_train_pred, y_test_pred))\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(15,5))\n",
    "    ax.set_ylim((-2.5,2.5))\n",
    "    ax.plot(predicted,label='predicted')\n",
    "    ax.plot(actual,label=\"actual\")\n",
    "    ax.axvline(x=y_train.shape[0],color='red')\n",
    "    ax.set_title(f'MLP results, red line denotes switch from train to test\\n{trainres}\\n{testres}')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8915da-c810-4e7a-bc8b-483ce0d2e55c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_and_report(mlp,\"MLP\", flattened_train32, flattened_test32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca72a54-46ac-45a8-96ec-00e8ed1b18f4",
   "metadata": {},
   "source": [
    "# Algorithm 2: A 1-D CNN\n",
    "\n",
    "## Preprocess the data\n",
    "The 2D CNN neeed to know the height and width of images in order to optimise its inner loops\n",
    "- look at my code from week 2 for an example\n",
    "- or [Keras.layers.Conv2d api](https://keras.io/api/layers/convolution_layers/convolution2d/)\n",
    "\n",
    "Similarly the 1D CNN layer needs to have a fixed size number of timesteps (sequences) to work with\n",
    "- not necessarily the same as the size of the filters (usually bigger)\n",
    "- but  it needs to know the size of the loop to run it's filters over\n",
    "\n",
    "\n",
    "So we can re-use the code we wrote to create the datasets \n",
    "- we'll change  the length of sequences we convolve over to 20 so convolution has something to work with\n",
    "- but we should consider the number of timesteps in a sequence as a hyper-paramter to optimise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d83b8c-9e7a-4b8a-b14b-1be6ea3650ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "cnn_train = train_series.batch(larger_batch_size,drop_remainder=True)\n",
    "\n",
    "cnn_test = test_series.batch(larger_batch_size,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423a3e1e-6b00-43dd-a283-4c5346a00f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape= list(cnn_test.take(1))[0][0].shape\n",
    "print(f'shape of batches is {batch_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ca3f8-c07e-4726-8944-38100e816e1a",
   "metadata": {},
   "source": [
    "## Now specify the 1D CNN architecture\n",
    "- Let's see how we get on with kernel size 3 (days) : another hyper-parameter to be tuned\n",
    "- we also need to specify the input shape which is (batch_size,1,sequence_length, num_features)  \n",
    "  i.e the shape of the batches we just found\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21d098-f256-4aca-b773-6ba2265bc479",
   "metadata": {},
   "outputs": [],
   "source": [
    "oneD_cnn = Keras.Sequential(\n",
    "                           [ Keras.layers.Conv1D(\n",
    "                                 filters=num_nodes,\n",
    "                                 kernel_size=5,\n",
    "                                 batch_input_shape=batch_shape,\n",
    "                                 activation='relu'\n",
    "                                 ),\n",
    "                            Keras.layers.Flatten(),\n",
    "                            Keras.layers.Dense(1,activation='linear')]\n",
    "            )\n",
    "oneD_cnn.compile(optimizer='adam', loss='mse')\n",
    "oneD_cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6047a-1744-4151-a756-95d7fd7137b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "history= oneD_cnn.fit(cnn_train, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d57a8b5-ccf5-4a60-91e3-d5dcb31c9667",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_report(oneD_cnn,\"1-D ConvNet\",cnn_train,cnn_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e5903-15cd-4415-a1f7-7d64c3afa4bf",
   "metadata": {},
   "source": [
    "# Algorithm 3 LSTM Network\n",
    "- For the LSTMs we will simply use our original dataset, \n",
    "- noting that we only have batchsize 1 which is slower, \n",
    "  but makes sense if we also preserve state between timesteps\n",
    "- getting the sequence length right would be a good start for experimentation\n",
    "\n",
    "### Warning: this is considerably (batch_size X) slower than MLP or CNN  \n",
    "\n",
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e112723-c976-45a9-b06b-2717ad1e0d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_train=  train_series\n",
    "lstm_test=  test_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1148d0-d35a-4ba2-b56b-b2265e1a0c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_shape = list(lstm_test.take(1))[0][0].shape\n",
    "print(f' for the lstm the batch shape is {batch_shape}' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f435b555-f079-4c96-b71d-ccac589cd744",
   "metadata": {},
   "source": [
    "## Define and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ff3ce-7584-43a3-8378-e31d584d447d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstmnet=Keras.Sequential(\n",
    "        [Keras.layers.LSTM(units=num_nodes,\n",
    "                           stateful=True,\n",
    "                           batch_input_shape=batch_shape),\n",
    "         Keras.layers.Dense(1)]\n",
    ")\n",
    "\n",
    "lstmnet.compile(optimizer='adam',loss='mse')\n",
    "history=lstmnet.fit(lstm_train,epochs=epochs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab1d37-ad3c-4e9f-886e-c9fc0a36cf83",
   "metadata": {},
   "source": [
    "## Evaluate and show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be5fac-5329-4b89-9bcd-54426f2769f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_and_report(lstmnet,\"LSTM\", lstm_train,lstm_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b456ab14-9511-4ebf-a301-42c40abbd0f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
