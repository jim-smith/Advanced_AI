{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbd0a96e-ac8e-4585-a9ce-eddc053e08d1",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Workbook on time-series prediction with different Neural Models\n",
    "\n",
    "## Aims:\n",
    "- to give you familiarity with using the keras DataSet class for time series data   \n",
    "   via the util function ```keras.utils.timeseries_dataset_from_array()```\n",
    "- to give you experience of configuring and running models that split between the  _feature encoders_ and a _prediction head_\n",
    "- to provider a springboard for you to explore different neural models  for sequence prediction problems\n",
    "\n",
    "## Note: Python Naming Convention\n",
    "- We've used a similar naming convention other than keras/tensorflow\n",
    "- i.e. `names_with_underscores_seperating_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ee97b0-1bd4-4f91-937d-990bba44a5a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import socket\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffdd6eff-8770-4eeb-aac8-94089d0f5680",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1: Introducing the keras [timeseries data set from array function](https://keras.io/api/preprocessing/timeseries)\n",
    "\n",
    "##  A simple example that fits on-screen\n",
    "To illustrate this, lets make a simple data set with four features and 10 time steps.\n",
    "\n",
    "To make it obvious what is happening, we'll assume:\n",
    "- the first feature has the values 1,2,...10, \n",
    "- the second feature has the values is 10,20,...\n",
    "- the third feature has the values 1000,2000 ... \n",
    "- and so on\n",
    "\n",
    "\n",
    "Now to make it easy to visualise on screen let's say:\n",
    "- we are using a window of size 3,\n",
    "- we are trying to predict the first column, one day in advance\n",
    "- i.e. at time _t_ \n",
    "  - we give the model x[t], x[t-1], and x[t-2] \n",
    "  we want to predict x[t+1][0] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d10054-8234-422b-82d1-c7d2c76f4b46",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Run the next cell to make the data in numpy and display it onscreen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6605f11-02a3-4e50-b78e-e52703f3b0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = np.zeros([10,4],dtype='int32')\n",
    "for row in range(10):\n",
    "    val=row+1\n",
    "    dataset[row][0] = val \n",
    "    dataset[row][1] = val*10\n",
    "    dataset[row][2] = val*100\n",
    "    dataset[row][3] = val*1000\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dd3c02-da91-462f-96cb-7e8dfbcce3d7",
   "metadata": {},
   "source": [
    "### 1.2 Now we will use a Keras built in function to give us a tensorflow dataset object\n",
    "\n",
    "Things to note:\n",
    "- they use the term _sequence_length_ as a synonym for *window size*\n",
    "- you define the target (y value to predict) by saying *how many rows* to look ahead and also *which column* to predict.\n",
    "- you can define how many styeps  forwards ot take between samples (the *stride* - same as for 2D convolutional networks)\n",
    "- for now we will set the *batch_size* to be 1 - later we w ill see how to change this dynamically\n",
    "- **you will normally set shuffle=False** - otyhereise you lose the temporal relationships in the data.\n",
    "\n",
    "**Run the cell below to create the Keras Dataset object**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4605adfd-73ba-4309-ba9a-1f6f2f56a09d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_size = 3\n",
    "timeseries = keras.utils.timeseries_dataset_from_array(\n",
    "    dataset,\n",
    "    sequence_length=window_size, # lets take sequences of length 3\n",
    "    targets=dataset[window_size:,0],# we want to predict feature[0] on the next input after our sequence\n",
    "    sequence_stride=1,\n",
    "    sampling_rate=1,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    seed=None,\n",
    "    start_index=None,\n",
    "    end_index=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1912c20-3c0f-42d7-b04b-876c4884a376",
   "metadata": {},
   "source": [
    "## What's in a dataset?\n",
    "Lots of stuff and lots of functionality!\n",
    "\n",
    "Datasets are designed to be used as part of a production pipeline.  \n",
    "So instead of getting access via indexes (lie you would for pandas or numpy), they\n",
    "provide access via iterators  such as _batch()_ or _take()_\n",
    "\n",
    "For now, it's easiest to look at what this dataset object contains if we convert it to a list then print it out.\n",
    "\n",
    "**Note** This code throws out a warning that it is reaching the end of the sequence and cannot process any more data.\n",
    "- This seems to be perfectly normal - ther eis lots of discussion online about it.\n",
    "- setting the wartning flags to remove this warning does not seem to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbefa2d-f9d7-4e6b-b019-d1ce98172a6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list(timeseries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9bc968-5e85-49f6-816a-1fb6df679ac8",
   "metadata": {},
   "source": [
    "### Yuk!\n",
    "We can just about see that in list form, the timeseries dataset  contains 7 pairs of items.\n",
    "- there are 7 because that is how many length 3 sequences you can get from 10 items\n",
    "\n",
    "Each item  has:\n",
    "- a tensor of shape (batchsize (1), sequence length (3), num_features(4))\n",
    "- a scalar value (again wrapped up inside a tensor) \n",
    "  - because we only asked to predict 1 feature (the one at index 0)\n",
    "  \n",
    "\n",
    "### For you to experiment\n",
    "Try  asking for\n",
    "- different length sequences (line 1)\n",
    "- or different size batches (line 8), \n",
    "- or for more than one feature as a label(line 5) \n",
    "\n",
    "then re-running the two code cells above and make sure you understand what you are getting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10afde6-7c29-48c7-8c37-73ee9ba0acef",
   "metadata": {},
   "source": [
    "## But a tensorflow dataset does have some advantages\n",
    "- for example we can ask it batch up the data\n",
    "- and if we pass it to tensorflow preprocessing layers or a model's fit() method they will do that\n",
    "\n",
    "So let's ask our timeseries to give us a load of batches\n",
    "\n",
    "Note that batch() or take() give us the outputs of type batchdataset so we have to iterate over their contents using\n",
    "````\n",
    "    for item in timeseries.batch(batch_size=1):\n",
    "    ````\n",
    "    \n",
    "instead of using slices like we would for numpy arrays or pandas dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7280ca-9903-4bf4-8b3d-af0e4fd40ee1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('First batches of size 1')\n",
    "for item in timeseries.batch(batch_size=1):\n",
    "    print(f'{item[:-1]} : {item[-1]}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0a0502-0488-453b-9fea-be423ccfbaa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('Now batches of size 2')\n",
    "x2= timeseries.batch(batch_size=2)\n",
    "for item in x2:\n",
    "    print(f'x= {item[:-1]} \\n y= {item[-1]}')\n",
    "print('The last batch only has one thing in, because there are only 7 sequences of length 3 in 0...9') "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8816645b-4019-43bd-a5b5-e64ce75cabaf",
   "metadata": {},
   "source": [
    "## Finally, this is one way to convert one of these items back to numpy using a lambda function\n",
    "\n",
    "Lets take the first batch as an example and turn it into a 'windowed' row of size (sequence length *number of features). and a scalar (the label)\n",
    "\n",
    "As you can see the function *flatten_dummy_sequence()* f uses some reshaping and indexing into the tensors.\n",
    "\n",
    "This can take a while to get your head around, so it's useful to implement and test your code with *toy* data designed so you can easily spot if you are pulling out the right things.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154771e5-7f4a-4494-8700-a9020cc389f3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flatten_dummy_sequence(item):\n",
    "    # get size of array to hold one windowed row\n",
    "    item_shape= item[0].shape\n",
    "    num_windowed_features= item_shape[-2]*item_shape[-1]\n",
    "    X= tf.reshape(item[0],num_windowed_features)\n",
    "    y = tf.reshape(item[1],1)\n",
    "    return(X,y)\n",
    "\n",
    "my_iterator = iter(timeseries)\n",
    "first_item = my_iterator.get_next()\n",
    "X,y = flatten_dummy_sequence(first_item) \n",
    "print(f'when flattened the first item is:\\n {X} : {y}')\n",
    "X,y = flatten_dummy_sequence(my_iterator.get_next())\n",
    "print(f'when flattened the next item is;\\n {X} : {y}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd7876f-2499-4e24-81d4-31fbb5f56ae2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 2: Comparing three different neural architectures for a time-series prediction problem\n",
    "\n",
    "## Data set description and characteristics\n",
    "Delhi data from [here](https://www.kaggle.com/datasets/sumanthvrao/daily-climate-time-series-data)\n",
    "\n",
    "4 variables: temp, humidity, windspeed, pressure\n",
    "Training set: \n",
    "\n",
    "\n",
    "## 2.1 Start by loading the data and cleaning up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da667a2-484d-4d02-b226-d3cc7472aa91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set path to data\n",
    "if (socket.gethostname()=='csctcloud'): #on csctcloud\n",
    "    datapath=\"/home/common/datasets\"\n",
    "elif (socket.gethostname()[0:7]=='jupyter'): #on csctcloud\n",
    "    datapath=\"~/shared/datasets\"\n",
    "else: #machine specific- this is for jim's development\n",
    "    datapath = \"../datasets\"\n",
    "reldirname= datapath +\"/delhi/\"\n",
    "\n",
    "\n",
    "#load data into pandas dataframes\n",
    "train_raw = pd.read_csv(reldirname +\"DailyDelhiClimateTrain.csv\")\n",
    "test_raw =  pd.read_csv(reldirname +\"DailyDelhiClimateTest.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4804e66-bd28-4613-9166-0e7d4914424c",
   "metadata": {},
   "source": [
    "### Print some descriptive statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409bf06b-aa3a-49d5-b62a-c0b25df212bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print some statistics\n",
    "print (f'training data set has {train_raw.shape[0]} '\n",
    "       f'rows and {train_raw.shape[1]} features\\n'\n",
    "       f' it has {train_raw.isna().sum().sum()} nulls\\n'\n",
    "      f'    text data set has  {test_raw.shape[0]} '\n",
    "       f'rows and {test_raw.shape[1]} features\\n'\n",
    "       f' it has {train_raw.isna().sum().sum()} nulls\\n'\n",
    "      )\n",
    "\n",
    "print(f'column names are {train_raw.columns}')\n",
    "\n",
    "#easier to read and use the pandas index item if we convert it to a list\n",
    "print(f' or more nicely when converted from pandas index to a list:\\n {list(train_raw.columns)}')\n",
    "\n",
    "#use the pandas function to p;rlovide stats abut the numerical features in  the raw data\n",
    "train_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9128ed-bbd9-48e5-ad6b-d4c38dd3ecc2",
   "metadata": {},
   "source": [
    "### Identify and fix the outliers\n",
    "What stands out immediately is that there are some wierd outliers in the meanPressure column.\n",
    "\n",
    "If it turns out that these are isolated odd readings we will replace them by the 50th centile value (~the mode)\n",
    "\n",
    "It's cleanest to do this using the pandas ```.loc[row,col]``` syntax   \n",
    " using a condition for the row \n",
    " - look this up if you've not come across it before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffc55e3-b0be-4a09-ab62-a917df35c0f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "median = train_raw['meanpressure'].median()\n",
    "\n",
    "# set values above 1200 mbar to the median value\n",
    "train_raw.loc[ train_raw['meanpressure'] >1200, 'meanpressure'] = median\n",
    "\n",
    "# set values below 800 mbar to the median value\n",
    "train_raw.loc[ train_raw['meanpressure'] <800, 'meanpressure'] = median\n",
    "\n",
    "train_raw.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cbdc3a-654a-433d-938e-6864c2d4b6bc",
   "metadata": {
    "tags": []
   },
   "source": [
    "###  For now just predict the next day's temperature. \n",
    "\n",
    "We use the pandas built in ```shift()``` method to move things downards for a given number of periods\n",
    "- 1 in our case, as we are predicting the next day\n",
    "\n",
    "Then we will need to fill in something sensibel for day1 where we had no target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72ecd2d-3190-47e7-b008-336ad3012a7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('This is the first few rows of raw training data\\n'\n",
    "      f'{train_raw.head()}')\n",
    "\n",
    "#select the mean temp to be the y value\n",
    "#and copy with the date value\n",
    "train_y = train_raw[['date','meantemp']]\n",
    "train_y.set_index('date', inplace=True)\n",
    "\n",
    "#shift works nicely if the index is a datetime object\n",
    "train_y =train_y.shift(periods=1)\n",
    "\n",
    "#fill NaN in first row with something sensible\n",
    "train_y.iloc[0] = train_raw['meantemp'].mean()\n",
    "\n",
    "\n",
    "print('\\nthis is the labels (y-data) showing how we have manipulated it\\n'\n",
    "      f'{train_y.head()}'\n",
    "     )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48647e39-bf69-4c12-a2bc-79673ba37b16",
   "metadata": {},
   "source": [
    "## 2.2 Getting the data ready for ML with Keras \n",
    "\n",
    "### Start by making a time series dataset from the train and test data we loaded\n",
    "with the aim of predicting the next day's temperature.   \n",
    "- for simplicity we'll drop the date columns and change everything to numpy arrays\n",
    "- we will leave out all the parameters where the default settings are fine\n",
    "- we'll take a window size of 7 days in case there are weekly effects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3352f1-a203-4210-8d3d-65a47330a53d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if 'date' in train_raw.columns:\n",
    "    train_raw=train_raw.drop(columns=['date'])\n",
    "if 'date' in test_raw.columns:\n",
    "    test_raw=test_raw.drop(columns=['date'])\n",
    "train=train_raw.to_numpy()\n",
    "test = test_raw.to_numpy()\n",
    "\n",
    "print(f'train and test shape {train.shape} , {test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e6a48a-cf0b-4ab4-ba0d-fcccc7adf8ab",
   "metadata": {},
   "source": [
    "### next we'll apply a standard scaler to transform values \n",
    "Note how we fit the scaler to the training data only (because the test data is *unseen*),  \n",
    "but apply the scaler to both training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb6fe57-e3a3-447d-9eab-869f7b9a9d11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "train=scaler.fit_transform(train)\n",
    "test=scaler.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7fa0e5-07d6-4678-9ccb-b8177df8bd5d",
   "metadata": {},
   "source": [
    "### Set the window size\n",
    "- we'll choose 7 since in many thigns to do with human activity there are weekly cycles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c32461-0216-4558-8633-1c82fc89435d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "window_size=7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12598fe6-ad6d-4958-bb85-b798f0ac8062",
   "metadata": {},
   "source": [
    "### Finally lets make the basic timeseries datasets\n",
    "and afterwards store and print out the shape of each batch and item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a909af4-a901-4fe2-9f0e-2847987268f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_keras_series = keras.utils.timeseries_dataset_from_array(\n",
    "    train,\n",
    "    sequence_length=window_size, \n",
    "    targets=train[window_size:,0],\n",
    "    batch_size=1\n",
    "    )\n",
    "\n",
    "test_keras_series = keras.utils.timeseries_dataset_from_array(\n",
    "    test,\n",
    "    sequence_length=window_size, \n",
    "    targets=test[window_size:,0],\n",
    "    batch_size=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5499d20-11fa-4eb8-aa91-5ea04e669e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_item=iter(train_keras_series).get_next()\n",
    "batch_shape= list(first_item)[0].shape\n",
    "print(f'shape of batches is {batch_shape}')\n",
    "\n",
    "item_shape= batch_shape[1:]\n",
    "print(f'shape of items is {item_shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c3bfd1-b384-4107-8602-ee35e79b4f5f",
   "metadata": {},
   "source": [
    "## 2.3 Define Some common things to use in our comparisons\n",
    "- If you were doing this *for real* you  would probably define these via a dictionary,    \n",
    "  so you could iterate over different values in code to find the best hyper-parameters for each algorithm\n",
    "- but we'll leave  that for your self-study\n",
    "\n",
    "What we will do is use the *pipeline* workflow \n",
    "- and define a common *regression head* \n",
    "- this will sit on top of the different algorithm *bodies*\n",
    "- and have one dense layer, then a single output node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77ae8f4-b5f4-4abe-b703-1ddc9f3c2c0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "epochs=15\n",
    "batch_size=20\n",
    "first_layer_nodes=20\n",
    "dense_nodes= 10\n",
    "\n",
    "regression_head = keras.Sequential([    \n",
    "    keras.layers.Dense(dense_nodes,activation='relu'),\n",
    "    keras.layers.Dense(1,activation='linear')])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1e3a6c-df4f-492d-b79f-0877ad53e8d9",
   "metadata": {},
   "source": [
    "### Finally define a performance reporting function \n",
    "- using a neat bit of code from [stackoverflow](https://stackoverflow.com/questions/56226621/how-to-extract-data-labels-back-from-- - Is this assumption about the appropriate error metric right???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf1c2e0-9e6c-4232-a6d8-193341352bcc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_and_report(model, name,train_ds,test_ds):\n",
    "    ''' gets the train and test mse error\n",
    "        for a given model and train/test datasets\n",
    "        and make a nice plot\n",
    "        Parameters:\n",
    "        ==========\n",
    "        model: trained instance of Keras Sequential or Model class\n",
    "        name: string to use for reporting\n",
    "        train_ds: ndarray or Keras dataset\n",
    "        test_ds: ndarray or keras dataset\n",
    "        '''\n",
    "    trainres=f'Training MSE= {model.evaluate(train_ds)}'\n",
    "    testres=f'Test MSe= {model.evaluate(test_ds)}'\n",
    "\n",
    "    y_train = np.concatenate([y for x, y in train_ds], axis=0)\n",
    "    y_train_pred= model.predict(train_ds).reshape(y_train.shape[0])\n",
    "    print('made training predictions')\n",
    "    \n",
    "    y_test = np.concatenate([y for x, y in test_ds], axis=0)\n",
    "    y_test_pred= model.predict(test_ds).reshape(y_test.shape[0])\n",
    "    print('made test predictions')\n",
    "    actual = np.concatenate((y_train,y_test))\n",
    "    predicted= np.concatenate((y_train_pred, y_test_pred))\n",
    "\n",
    "    fig,ax = plt.subplots(figsize=(15,5))\n",
    "    ax.set_ylim((-2.5,2.5))\n",
    "    ax.plot(predicted,label='predicted')\n",
    "    ax.plot(actual,label=\"actual\")\n",
    "    ax.axvline(x=y_train.shape[0],color='red')\n",
    "    ax.set_title(f'{name} results, red line denotes switch from train to test\\n{trainres}\\n{testres}')\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830c3da7-d933-4e7e-b017-24c7fbd93638",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2.4 Algorithm 1: a MLP with a time window.\n",
    "\n",
    "### Preprocess data\n",
    "For this case we can 'flatten' each X item the timeseries dataset from a windowsizex4 array into a 12x1\n",
    "\n",
    "- I've been a bit lazy and worked out the flat size in advance\n",
    "- and i've just printed out the first item from the dataset   \n",
    "  to illustrate the dataset.take() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7aa6ca-fd4d-4dd1-bf42-6c161ccc5e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flatsize = window_size* 4 #window size * num features\n",
    "\n",
    "def flatten_weather_sequence(X,y):\n",
    "    return(tf.reshape(X,[1,flatsize]),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0627d10c-9c58-48e9-b794-b2bec400c66a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flattened_train = train_keras_series.map(flatten_weather_sequence)\n",
    "flattened_test = test_keras_series.map(flatten_weather_sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8eae52b-4da0-42fc-8842-cab2f5b39e31",
   "metadata": {},
   "source": [
    "### let's examine the sizes of the tensors holding the examples and batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6168f612-49e8-408c-9d11-a102436e2da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get a batch \n",
    "first_item= iter(flattened_train).get_next()\n",
    "print(f' first_item is of type {type(first_item)}\\n'\n",
    "      f'with contents {first_item}\\n'\n",
    "     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812b43dc-36df-477e-95a8-1bb3702c80ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "flattened_batch_shape = list(first_item)[0].shape\n",
    "print( f'shape of batches in flattened version is now {flattened_batch_shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f5d897-80ae-459d-908b-47b1929f2d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'batchsize {flattened_batch_shape[0]}, item shape {flattened_batch_shape[1:]}')\n",
    "flattened_item_shape = flattened_batch_shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d05e7b-e3ad-4215-8710-08cc5433d5bf",
   "metadata": {},
   "source": [
    "### Now lets build a sequential model for our MLP\n",
    "- we'll use a single hidden layer of 20 nodes (we defined this in section 2.3) \n",
    "- note how in the fit() method we can now change batch shape from 1 to 20 on the fly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15469fa5-0144-4c82-b58f-4c5fbf966272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#define the body\n",
    "mlp_body= keras.Sequential(\n",
    "    [\n",
    "    keras.Input(shape=flattened_item_shape),\n",
    "    keras.layers.Dense(first_layer_nodes,activation='relu'),\n",
    "    ]\n",
    "    )\n",
    "       \n",
    "#add our standard prediction head on top\n",
    "mlp= keras.Sequential( [mlp_body, regression_head])\n",
    "\n",
    "# build summarise and train\n",
    "mlp.compile(optimizer='adam', loss='mse')\n",
    "mlp.summary()\n",
    "history=mlp.fit(flattened_train, epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5953a0f-d68f-411e-bc5b-9ad17c68b5c8",
   "metadata": {},
   "source": [
    "### Let's see how well it did on the training and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8915da-c810-4e7a-bc8b-483ce0d2e55c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_and_report(mlp,\"MLP\", flattened_train, flattened_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ca72a54-46ac-45a8-96ec-00e8ed1b18f4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## 2.5 Algorithm 2: A 1-D CNN\n",
    "\n",
    "## Preprocess the data\n",
    "The 2D CNN neeed to know the height and width of images in order to optimise its inner loops\n",
    "- look at my code from week 2 for an example\n",
    "- or [Keras.layers.Conv2d api](https://keras.io/api/layers/convolution_layers/convolution2d/)\n",
    "\n",
    "Similarly the 1D CNN layer needs to have a fixed size number of timesteps (sequences) to work with\n",
    "- not necessarily the same as the size of the filters (usually bigger)\n",
    "- but  it needs to know the size of the loop to run it's filters over\n",
    "\n",
    "So we can re-use the code we wrote to create the datasets which had sequences of length 7 days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59ca3f8-c07e-4726-8944-38100e816e1a",
   "metadata": {},
   "source": [
    "## Now specify the 1D CNN architecture\n",
    "- Let's see how we get on with kernel size 3 (days) : another hyper-parameter to be tuned\n",
    "- and for fairness with the MLP (which had 20 hidden nodes) we'l have 20 kernels\n",
    "- we also need to specify the input shape which is (batch_size,1,sequence_length, num_features)  \n",
    "  i.e the shape of the batches we just found\n",
    "\n",
    "** The main thing to note with ```Conv1D``` layers**\n",
    "- is that for sequence problems we combine the kernel outputs using a ```GlobalAveragePooling1D()```\n",
    "- instead of a ```Flatten()``` layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed21d098-f256-4aca-b773-6ba2265bc479",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "oneD_cnn_body = keras.Sequential(\n",
    "                           [ keras.Input(shape=item_shape),\n",
    "                             keras.layers.Conv1D(\n",
    "                                 filters= first_layer_nodes,\n",
    "                                 kernel_size=3,\n",
    "                                 activation='relu'\n",
    "                                 ),\n",
    "                            keras.layers.GlobalAveragePooling1D(),\n",
    "                            ]\n",
    "            )\n",
    "oneD_cnn= keras.Sequential (\n",
    "    [oneD_cnn_body,\n",
    "     regression_head\n",
    "    ] )\n",
    "oneD_cnn.compile(optimizer='adam', loss='mse')\n",
    "oneD_cnn.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6047a-1744-4151-a756-95d7fd7137b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history= oneD_cnn.fit(train_keras_series, epochs=epochs,batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5441a4-f140-44da-9ef7-19655c774d31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_and_report(oneD_cnn,\"1-D ConvNet\",train_keras_series,test_keras_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d2e5903-15cd-4415-a1f7-7d64c3afa4bf",
   "metadata": {},
   "source": [
    "## 2.6 Algorithm 3 LSTM Network\n",
    "- For the LSTMs we will simply use our original dataset, \n",
    "- getting the sequence length right would be a good start for experimentation\n",
    "\n",
    "**Notice** that this is considerably  slower than MLP or CNN  \n",
    "- because it is having to do BackPropagation Thought Time\n",
    "- the memory overhead is also bigger\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f435b555-f079-4c96-b71d-ccac589cd744",
   "metadata": {},
   "source": [
    "## Define and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67ff3ce-7584-43a3-8378-e31d584d447d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lstm_body=keras.Sequential(\n",
    "        [keras.Input(shape=item_shape),\n",
    "         keras.layers.LSTM(units=first_layer_nodes,\n",
    "                           stateful=False,\n",
    "                          ),\n",
    "        ])\n",
    "\n",
    "lstmnet= keras.Sequential( [lstm_body,regression_head]) \n",
    "\n",
    "\n",
    "lstmnet.compile(optimizer='adam',loss='mse')\n",
    "history=lstmnet.fit(train_keras_series,epochs=epochs,batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ab1d37-ad3c-4e9f-886e-c9fc0a36cf83",
   "metadata": {},
   "source": [
    "## Evaluate and show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40be5fac-5329-4b89-9bcd-54426f2769f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_and_report(lstmnet,\"LSTM\", train_keras_series,test_keras_series)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ef2f54-f084-4807-82b0-73936c94184b",
   "metadata": {},
   "source": [
    "# Part 3: For you to experiment\n",
    "\n",
    "To make sure you are familiar with using the TimeSeriesDataset class and algorithms you could experiment with:\n",
    "- Loking at the outputs as the model train: is it worth changing the maximum  number of epochs allowed?\n",
    "- Changing the number of feature detectors (hidden layer perceptrons - 1DConv filters - LSTM nodes) in the models\n",
    "- different datasets from kaggle etc.\n",
    "- creating a benchmark algorithm with prediction='same as last time step'\n",
    "- doing a fairer algorithm comparison via hyper-parameter tuning\n",
    "- finding ways of monitoring memory usage\n",
    "    - can you do this without requiring jupyter extensions you not be able to install in a work environment?\n",
    "- investigating what sorts of errors different models make\n",
    "  - starting by improving the plotting function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16123c99-613f-4bbe-a914-3ddfe3ebc885",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
