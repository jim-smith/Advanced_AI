{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 3: Text Classification\n",
    "\n",
    "In the previous practicals we created some IMDB movie review data for sentiment analysis and explored several text pre-processing and representation methods. By now you should have pre-processed the reviews you scraped from IMDB and also the full 50,000 review dataset. So, now we are ready to train a model to classify the sentiment of our movie reviews! We will explore several unsupervised and supervised approaches, using an existing movie review dataset for training and keep ours as an additional test set.\n",
    "\n",
    "In the first part of this practical we will explore two supervised classification algorithms, Naive Bayes and an Artificial Neural Network (ANN).\n",
    "\n",
    "In the second part of this practical we will look at several unsupervised algorithms K-means clustering and Semantic Analysis using word embeddings.\n",
    "\n",
    "The objectives of this practical are:\n",
    "\n",
    "1. Apply a complete NLP workflow for text classification\n",
    "\n",
    "2. Understand the probabilistic Naive Bayes classifier and consider different aspects of applying an ANN to text data\n",
    "\n",
    "3. Consider appropriate representations for unsupervised text classification, including clustering and semantic analysis with word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Supervised Text Classification\n",
    "\n",
    "## 1.0 Import libraries\n",
    "\n",
    "1. [Tensorflow](https://www.tensorflow.org/) - is a powerful Python library for machine learning.\n",
    "\n",
    "2. [Keras](https://keras.io/) - is a simple API for building machine learning models and is built into Tensorflow 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the directory to the data folder\n",
    "data_dir = os.path.join('..', 'data', 'imdb')\n",
    "\n",
    "# Spacy needs to install the language model also\n",
    "# If you recieve an error, uncomment the following line and re-run the cell\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First load the full IMDB dataset and our smaller reviews set.\n",
    "\n",
    "2. Then we need to convert the 'positive' and 'negative' class labels to numerical values, 1 for positive and 0 for negative. Using the pandas `get_dummies` function creates two binary valued columns and then the `drop_first` parameter collapses these into a single column.\n",
    "\n",
    "3. The next cell plots the distribution of reveiew sentiment for the dataset and our reviews. As you can, see the classes are prefectly balanced within the dataset, but are they in your data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imdb dataset\n",
    "imdb_data = pd.read_csv(os.path.join(data_dir, 'imdb_dataset.csv'))\n",
    "\n",
    "# Load your imdb reviews\n",
    "imdb_reviews = pd.read_csv(os.path.join(data_dir, 'imdb_reviews.csv'), index_col=0)\n",
    "# Just keep the review text and the sentiment columns\n",
    "imdb_reviews = imdb_reviews[['review', 'sentiment']]\n",
    "\n",
    "# Convert the sentiment to a binary value\n",
    "imdb_data['sentiment'] = pd.get_dummies(imdb_data['sentiment'], drop_first=True)\n",
    "imdb_reviews['sentiment'] = pd.get_dummies(imdb_reviews['sentiment'], drop_first=True)\n",
    "\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax1 = sns.countplot(imdb_data, x='sentiment', ax=ax[0])\n",
    "ax1 = sns.countplot(imdb_reviews, x='sentiment', ax=ax[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the vocabulary\n",
    "\n",
    "From last week you should have created a vocabulary from the larger IMDB dataset, so lets load that and set the vocabulary size accordingly. We will also add some special padding and unknown tokens for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary file and store each word in a list\n",
    "with open(os.path.join(data_dir, 'imdb_vocab.txt'), 'r') as file:\n",
    "    imdb_vocab = file.read().splitlines() \n",
    "    \n",
    "# Set the vocab size\n",
    "vocab_size = len(imdb_vocab)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary size: \" + str(vocab_size))\n",
    "for i, word in enumerate(imdb_vocab[:50]):\n",
    "    print(f'({str(i)}, {word})', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and vectorise the text\n",
    "\n",
    "1. We will use sklearns `CountVectorizer()` to tokenise the text and vectorise each review into a BOW.\n",
    "\n",
    "2. Once the text is vectorised, split into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CountVectorizer\n",
    "bow_vectoriser = CountVectorizer(max_features=vocab_size)\n",
    "\n",
    "# Vectorise the text\n",
    "X = bow_vectoriser.fit_transform(imdb_data['review']).toarray()\n",
    "print('Shape of X:', X.shape)\n",
    "print(X[:5, :])\n",
    "\n",
    "# Get the class labels\n",
    "y = imdb_data['sentiment'].values\n",
    "print('Shape of y:', y.shape)\n",
    "print(y[:5])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Naive Bayes\n",
    "\n",
    "Naïve Bayes is a generative classification algorithm which finds the probability of an event based on prior knowledge/examples of similar events. It is naïve, because it assumes that each feature is independent (do not effect each other) so we can calculate probabilities independently.\n",
    "\n",
    "The class defined below implements the following formulation of the algorithm:\n",
    "\n",
    "$ \\hat{y} = argmax(log(P(y) + \\sum_{i=1}^{n} P(x_i|y))) $\n",
    "\n",
    "Where:\n",
    "\n",
    "$ \\hat{y} = $ the predicted label\n",
    "\n",
    "$ P(y) = $ the probability of class y\n",
    "\n",
    "$ P(x_i|y) = $ the product of the probability that feature i in x occurs, given y\n",
    "\n",
    "By default the algorithm uses:\n",
    "- a laplace smoothing parameter `alpha=1.0`, which prevents division by zero when calculating likelihoods for words that do not appear in the training data for a given class.\n",
    "- and `use_log=True`, to calculates probabilities in log space which numerically more stable.\n",
    "\n",
    "<div class = \"alert alert-block alert-info\"><b>Note:</b> For a thorough discussion of Naïve Bayes, including smoothing, logs and several Python implementations see <a href='https://sidsite.com/posts/implementing-naive-bayes-in-python/'> this </a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes():\n",
    "    \"\"\"Naive Bayes classifier for categorical data.\"\"\"\n",
    "\n",
    "    def __init__(self, alpha=1.0, use_log=True):\n",
    "        \"\"\" Arguments:\n",
    "                alpha: Laplace smoothing parameter.\n",
    "                use_log: Use log probabilities to avoid underflow.\n",
    "        \"\"\"\n",
    "        self.alpha = alpha # Smoothing parameter. Prevents division by zero when calculating likelihoods.\n",
    "        self.use_log = use_log # Use log probabilities to avoid underflow.\n",
    "        self.prior = None # The prior (mu) distribution of class labels. The probability of each class, P(class) within the training data.\n",
    "        self.multinomial = None # The multinomial distribution (phi) is the probability/likelihood of each feature conditioned on the class, P(feature | class).\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit training data for Naive Bayes classifier.\"\"\"\n",
    "\n",
    "        # N is the number of examples\n",
    "        N = X.shape[0]\n",
    "\n",
    "        # Calculate prior\n",
    "        # Split the input array into sub-arrays depending on class label\n",
    "        X_by_class = np.array([X[y == class_lbl] for class_lbl in np.unique(y)], dtype=object)\n",
    "        \n",
    "        # Count the number of examples in each class and divide by total number of examples\n",
    "        self.prior = np.array([X_class.shape[0] / N for X_class in X_by_class])\n",
    "        assert len(self.prior) == len(np.unique(y)), 'Number of priors should equal number of classes'\n",
    "\n",
    "        # Calculate multinomial coefficients\n",
    "        # Create array of shape (num_classes, num_features) to hold multinomial coefficients\n",
    "        self.multinomial = np.zeros((len(np.unique(y)), X.shape[1]))\n",
    "\n",
    "        for class_lbl in range(len(self.prior)):\n",
    "\n",
    "            # Count the number of times each feature appears in all examples of a particular class + alpha\n",
    "            class_feature_counts = X_by_class[class_lbl].sum(axis=0) + self.alpha\n",
    "\n",
    "            # Probability of each feature given the class\n",
    "            # Individual feature counts divided by the total number of times all features appear in the class\n",
    "            self.multinomial[class_lbl] = class_feature_counts / class_feature_counts.sum()\n",
    "        \n",
    "        # Convert to log probabilities\n",
    "        if self.use_log:\n",
    "            self.prior = np.log(self.prior)\n",
    "            self.multinomial = np.log(self.multinomial)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Predict probability of class for each input example.\"\"\"\n",
    "\n",
    "        # Create array of shape (num_examples, num_classes) to store class probabilities (posterior) for each example\n",
    "        class_probabilities = np.zeros(shape=(X.shape[0], self.prior.shape[0]))\n",
    "\n",
    "        # Loop over each example and calculate individual conditional likelihoods for each class,\n",
    "        # then multiply them all together (the product), and multiply by the class priors,\n",
    "        # or in log space, add them all together (the sum), and add the class priors.\n",
    "        for i, example in enumerate(X):\n",
    "            example_likelihood = []\n",
    "\n",
    "            # Loop over each class\n",
    "            for class_lbl in range(len(self.prior)):\n",
    "                feature_likelihood = []\n",
    "\n",
    "                # Loop over each feature\n",
    "                for feature in range(example.shape[0]):\n",
    "                    # If the feature is present in the example\n",
    "                    if example[feature] > 0:\n",
    "                        # Calculate the probability of the feature given the class (multinomial coefficient */+ feature count)\n",
    "                        mn_coefficient = self.multinomial[class_lbl][feature]\n",
    "                        # If using log space the convert the example feature count to log space\n",
    "                        if self.use_log:\n",
    "                            feature_likelihood.append(mn_coefficient + np.log(example[feature]))\n",
    "                        else:\n",
    "                            feature_likelihood.append(mn_coefficient ** example[feature])\n",
    "\n",
    "                # Append the probabilties of this class for this example\n",
    "                example_likelihood.append(feature_likelihood)\n",
    "\n",
    "            # Calculate joint probabilities\n",
    "            # Multiply (or sum) all the individual feature probabilities together and multiply by (or add) class priors\n",
    "            if self.use_log:\n",
    "                class_probabilities[i] = np.asarray(example_likelihood).sum(axis=1) + self.prior\n",
    "            else:\n",
    "                class_probabilities[i] = np.asarray(example_likelihood).prod(axis=1) * self.prior\n",
    "        \n",
    "        # Normalise so probabilities sum to 1\n",
    "        class_probabilities = class_probabilities / np.linalg.norm(class_probabilities, ord=1, axis=1, keepdims=True)\n",
    "        assert (class_probabilities.sum(axis=1) - 1 < 0.001).all(), 'Rows should sum to 1'\n",
    "\n",
    "        return class_probabilities\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict class with highest probability.\"\"\"\n",
    "        return self.predict_proba(X).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a Naive Bayes classifier\n",
    "nb = NaiveBayes()\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "# Predict class labels for validation set\n",
    "predictions = nb.predict(X_val)\n",
    "print('Accuracy:', accuracy_score(y_val, predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = ConfusionMatrixDisplay.from_predictions(y_val, predictions, display_labels=['negative', 'positive',], colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on your IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorise the text\n",
    "X_test = bow_vectoriser.transform(imdb_reviews['review']).toarray()\n",
    "\n",
    "# Get the class labels\n",
    "y_test = imdb_reviews['sentiment'].values\n",
    "\n",
    "# Predict class labels for test set\n",
    "predictions = nb.predict(X_test)\n",
    "print('Test Accuracy:', accuracy_score(y_test, predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = ConfusionMatrixDisplay.from_predictions(y_test, predictions, display_labels=['negative', 'positive',], colorbar=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Exercise: Understanding Naive Bayes and evaluating pre-processing\n",
    "\n",
    "1. Take some time to examin the code in the `NaiveBayes()` class defined above. You should ensure you understand what is happening in the `fit()` and `predict_proba()` methods and how that relates to the equation for the Naive Bayes algorithm.\n",
    "\n",
    "2. We also used the datasets that were already pre-processed. However, the `CountVectorizer()` has some built in pre-processing methods to strip accents, lowercase and remove stop words. Try loading the unprocessed datasets and experimenting with the build in options. Does your pre-processed data result in an improvement?\n",
    "\n",
    "3. Sklearn also has several implementations of Naive Bayes. Try comparing the implementation above to the `MultinomialNB()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Artificial Neural Network\n",
    "\n",
    "ANN are a discriminative classification algorithm which learn a decision boundary to separate the classes and select appropriate labels based on the input features.\n",
    "\n",
    "The cell below creates a Keras sequential model:\n",
    "- The `input_shape` of the input layer is the size of the vocabulary, because each word is a feature.\n",
    "\n",
    "- It uses two hidden layers with relu activation and intermediate Dropout layers to prevent overfitting.\n",
    "\n",
    "- The output layer only needs one node, because this is a binary classification problem.\n",
    "\n",
    "- Finally, we compile using the adam optimiser and binary cross-entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequential model\n",
    "model = models.Sequential()\n",
    "\n",
    "# Input layer\n",
    "model.add(layers.Dense(40, activation=\"relu\", input_shape=(vocab_size, )))\n",
    "# Hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(20, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(10, activation=\"relu\"))\n",
    "# Output layer\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model\n",
    "\n",
    "Now train the model for a few epochs and evaluate on the test set. You should see an improvement over the Naive Bayes model. You can compare your models performance to the 'state-of-the-art' listed [here](https://paperswithcode.com/sota/sentiment-analysis-on-imdb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "results = model.fit(X_train, y_train, epochs=2, batch_size=500, validation_data=(X_val, y_val))\n",
    "print(\"Validation Accuracy:\", round(results.history[\"val_accuracy\"][-1], 3))\n",
    "\n",
    "# Predict class labels for test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = [0 if x < 0.5 else 1 for x in predictions] # Convert probabilities to binary\n",
    "print('Test Accuracy:', accuracy_score(y_test, predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = ConfusionMatrixDisplay.from_predictions(y_test, predictions, display_labels=['negative', 'positive',], colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Exercise: Different model parameters and input representations\n",
    "\n",
    "1. Experiment with different numbers of nodes within the ANN layers, a numbers of layers, dropout probabilities and optimisation algorithms to see what impact they have on model performance.\n",
    "\n",
    "2. You may have also noticed that currently the model is only trained for two epochs. Try a different number and observe the effect.\n",
    "\n",
    "3. This model works well with BOW inputs. Try TF-IDF to see if it improves performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Unsupervised Text Classification\n",
    "\n",
    "## 2.0 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the directory to the data folder\n",
    "data_dir = os.path.join('..', 'data', 'imdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imdb dataset\n",
    "imdb_data = pd.read_csv(os.path.join(data_dir, 'imdb_dataset.csv'))\n",
    "\n",
    "# Load your imdb reviews\n",
    "imdb_reviews = pd.read_csv(os.path.join(data_dir, 'imdb_reviews.csv'), index_col=0)\n",
    "# Just keep the review text and the sentiment columns\n",
    "imdb_reviews = imdb_reviews[['review', 'sentiment']]\n",
    "\n",
    "# Convert the sentiment to a binary value\n",
    "imdb_data['sentiment'] = pd.get_dummies(imdb_data['sentiment'], drop_first=True)\n",
    "imdb_reviews['sentiment'] = pd.get_dummies(imdb_reviews['sentiment'], drop_first=True)\n",
    "\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and vectorise the text\n",
    "\n",
    "Unlike our application of Naive Bayes and ANN, for clustering we need to change our input representations need to consider two things:\n",
    "- **Zero values** - in Naive Bayes these simply result in low probability and in an ANN inputs of 0 will not impact weight updates.\n",
    "- **Common words** - these have less impact on the probabilistic and ANN approach, but for clustering, since we are calculating the distance/similarity between examples, lots of common but uninformative words will tend to make examples appear closer/more similar.\n",
    "\n",
    "1. We will use sklearns `TfidfVectorizer()` to tokenise the text and vectorise each review into a TF-IDF values. This should reduce the magnitude of very common words, which are unlikely to provide sentiment information. We will also use 1-grams *and* bi-grams to better capture the relationships between word pairs.\n",
    "\n",
    "2. To avoid zeros within the input, and enhance important words, we will also scale the TF-IDF values.\n",
    "\n",
    "3. Next use PCA to select only the most important features. Here we use 2 so that the clusters can be plotted in 2D.\n",
    "\n",
    "2. Finally, split into training and validation sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TfidfVectorizer, StandardScaler and PCA\n",
    "tfidf_vectoriser = TfidfVectorizer(max_features=vocab_size, ngram_range=(1, 2))\n",
    "scaler = StandardScaler()\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Vectorise the text, scale it and apply PCA\n",
    "X = tfidf_vectoriser.fit_transform(imdb_data['review']).toarray()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pca.fit_transform(X)\n",
    "print('Shape of X:', X.shape)\n",
    "print(X[:5, :])\n",
    "\n",
    "# Get the class labels\n",
    "y = imdb_data['sentiment'].values\n",
    "\n",
    "print('Shape of y:', y.shape)\n",
    "print(y[:5])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 K-means\n",
    "\n",
    "K-means is a clustering algorithm which defines a set of centroids (mid-points) for each cluster.\n",
    "\n",
    "Examples are assigned to the closest cluster/centroid. Centroids are repeatedly moved to a new mid-point of all examples in the cluster, until no further changes are made.\n",
    "\n",
    "Pseudocode:\n",
    "```\n",
    "Set K number of centroids and randomly assign to examples\n",
    "Set converged = False\n",
    "\n",
    "WHILE not converged:\n",
    "\tFor each example i:\n",
    "\t\tFor each cluster k:\n",
    "\t\t\tCalculate distance(i, k)\n",
    "\t\tAssign i to cluster with smallest distance\n",
    "\t\n",
    "\tIF no examples moved cluster:\n",
    "\t\tconverged = True\n",
    "\tELSE:\n",
    "\t\tFor each cluster k:\n",
    "\t\t\tSET new cluster centroid = average position of examples in cluster\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train a Kmeans\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X_train)\n",
    "\n",
    "# Predict class labels for validation set\n",
    "predictions = kmeans.predict(X_val)\n",
    "print('Accuracy:', accuracy_score(y_val, predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = ConfusionMatrixDisplay.from_predictions(y_val, predictions, display_labels=['negative', 'positive',], colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model on your IMDB reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vectorise the text, scale it and apply PCA\n",
    "X_test = bow_vectoriser.transform(imdb_reviews['review']).toarray()\n",
    "X_test = scaler.transform(X_test)\n",
    "X_test = pca.transform(X_test)\n",
    "\n",
    "# Get the class labels\n",
    "y_test = imdb_reviews['sentiment'].values\n",
    "\n",
    "# Predict class labels for test set\n",
    "predictions = kmeans.predict(X_test)\n",
    "print('Test Accuracy:', accuracy_score(y_test, predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = ConfusionMatrixDisplay.from_predictions(y_test, predictions, display_labels=['negative', 'positive',], colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the decision boundary, cluster centroids and validation and test datapoints\n",
    "\n",
    "Code for plotting is adapted from [here](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_digits.html#sphx-glr-auto-examples-cluster-plot-kmeans-digits-py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = 0.01  # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh using trained model.\n",
    "Z = kmeans.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1)\n",
    "plt.clf()\n",
    "plt.imshow(\n",
    "    Z,\n",
    "    interpolation=\"nearest\",\n",
    "    extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "    # cmap=plt.cm.Paired,\n",
    "    aspect=\"auto\",\n",
    "    origin=\"lower\",\n",
    ")\n",
    "\n",
    "# Plot the validation and test data only\n",
    "points = np.concatenate((X_val, X_test))\n",
    "labels =  np.concatenate((y_val, y_test))\n",
    "\n",
    "pos_points = np.array([list(points[i]) for i in range(len(points)) if labels[i] == 1])\n",
    "neg_points = np.array([list(points[i]) for i in range(len(points)) if labels[i] == 0])\n",
    "\n",
    "plt.scatter(pos_points[:, 0], pos_points[:, 1], color=\"green\", s=2)\n",
    "plt.scatter(neg_points[:, 0], neg_points[:, 1], color=\"red\", s=2)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = kmeans.cluster_centers_\n",
    "plt.scatter(\n",
    "    centroids[:, 0],\n",
    "    centroids[:, 1],\n",
    "    marker=\"x\",\n",
    "    s=169,\n",
    "    linewidths=3,\n",
    "    color=\"w\",\n",
    "    zorder=10,\n",
    ")\n",
    "\n",
    "plt.title(\"K-means clustering on the IMDB sentiment data\")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Semantic Similarity\n",
    "\n",
    "We can use semantic representation of word embeddings to build a simple classifier using a heuristic approach.\n",
    "\n",
    "1. Start with a list(s) of words that are representative of the categories e.g. ‘good’ and ‘bad’ for sentiment.\n",
    "\n",
    "    - Optionally, find other similar words within the corpus using cosine similarity.\n",
    "\n",
    "2. Score each word in an input example according to its similarity to each word in the category lists.\n",
    "\n",
    "3. Average the word scores to produce semantic scores for each category.\n",
    "\n",
    "\n",
    "We will use gensim to load Word2Vec embeddings and fine tune to our corpus of IMDB reviews.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the reviews\n",
    "imdb_corpus = imdb_data['review'].apply(lambda x: [token.text for token in nlp.tokenizer(x)])\n",
    "\n",
    "# Create a word2vec model with gensim\n",
    "embedding_dim = 300\n",
    "w2v_model = Word2Vec(sentences=imdb_corpus, size=embedding_dim, window=5, min_count=5, sg=1, seed=1, workers=4)\n",
    "\n",
    "print(\"W2v model vocabulary: \" + str(list(w2v_model.wv.vocab)[:50]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a list of positive and negative sentiment words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a selection of positive and negative words\n",
    "pos_words = ['excellent', 'awesome', 'cool', 'decent', 'amazing', 'strong', 'good', 'great', 'funny', 'entertaining']\n",
    "neg_words = ['terrible', 'awful', 'horrible', 'boring', 'bad', 'disappointing', 'weak', 'poor', 'senseless', 'confusing']\n",
    "\n",
    "# Get the most similar words for each word in the positive and negative lists\n",
    "pos_sims = w2v_model.wv.most_similar(pos_words, topn=10)\n",
    "print('Positive similar words:')\n",
    "print(pos_sims)\n",
    "neg_sims = w2v_model.wv.most_similar(neg_words, topn=10)\n",
    "print('Negative similar words:')\n",
    "print(neg_sims)\n",
    "\n",
    "# Get the vectors for each word in the positive and negative lists\n",
    "pos_words.extend([word for word, score in pos_sims])\n",
    "pos_vectors = [w2v_model.wv.get_vector(word) for word in pos_words]\n",
    "\n",
    "neg_words.extend([word for word, score in neg_sims])\n",
    "neg_vectors = [w2v_model.wv.get_vector(word) for word in neg_words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the semantic score of each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function maps a word to its vector representation\n",
    "def tokens_to_vecs(tokens, model):\n",
    "    vecs = []\n",
    "    for word in tokens:\n",
    "        if word in model.wv.vocab:\n",
    "            vecs.append(w2v_model.wv.get_vector(word))\n",
    "    return vecs\n",
    "\n",
    "# Score each review based on the similarity between the positive and negative word vectors\n",
    "semantic_scores = np.zeros((len(imdb_corpus), 2 ))\n",
    "for i, review in enumerate(imdb_corpus):\n",
    "    # Get the vectors for each word in the review\n",
    "    review_tokens = tokens_to_vecs(review, w2v_model)\n",
    "    # Calculate the similarity betwen review vectors and positive/negative word vectors\n",
    "    pos_score = cosine_similarity(review_tokens, pos_vectors)\n",
    "    neg_score = cosine_similarity(review_tokens, neg_vectors)\n",
    "    # Take the average of the similarity scores\n",
    "    semantic_scores[i][0] = np.mean(pos_score)\n",
    "    semantic_scores[i][1] =  np.mean(neg_score)\n",
    "\n",
    "print('Semantic scores:', semantic_scores[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assign sentiment labels according to category with the highest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the scores and the class labels\n",
    "X = semantic_scores\n",
    "y = imdb_data['sentiment'].values\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Predict class labels for training set\n",
    "predictions = [1 if X_train[i][0] >  X_train[i][1] else 0 for i in range(len(X_train))]\n",
    "print('Train Accuracy:', accuracy_score(y_train, predictions))\n",
    "\n",
    "# Predict class labels for validation set\n",
    "predictions = [1 if X_val[i][0] >  X_val[i][1] else 0 for i in range(len(X_val))]\n",
    "print('Validation Accuracy:', accuracy_score(y_val, predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = ConfusionMatrixDisplay.from_predictions(y_val, predictions, display_labels=['negative', 'positive',], colorbar=False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Exercise: Different embedding sizes and category words\n",
    "\n",
    "1. Try repeating the semantic similarity classifier with different embedding dimensions and examine the impact on classification accuracy.\n",
    "\n",
    "2. Similarly try reducing the list of positive and negative words.\n",
    "\n",
    "3. You could also consider ignoring words with low positive/negative scores, because these tend to be less informative."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "eda59365f9d652723e3bcf67739b9100ac1f6ab6ddfa121c8653940903b971a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
