{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical 4: Recurrent Neural Networks\n",
    "\n",
    "For most NLP problems the input representations consist of sequences of words (sentences, paragrphs, etc). The words in a sequnce have a temporal relationship, or in other words, the *order* in which they appear changes the meaning of the sequence. In the previous practical we looked at several text classification methods which were unable to model this sequential relationship. Either because the algorithm processes all input features at once, or because the temporal relationship is lost through the chosen representation (BOW/TF-IDF), or both.\n",
    "\n",
    "In contrast, RNN process each feature of an input exaple in turn, at each timestep. So each word of an input sequnce is processed and used to update an internal *hidden state*, or memory, of the network. Thus RNN are able to capture temporal relationships between each feature of an input example. For this reason RNN are frequently used form many NLP tasks, such as classification and language modelling.\n",
    "\n",
    "In the first part of this practical we will apply an RNN (LSTM) to the task of sentiment classifcation on the IMDB dataset. We will use word embedding representations for each word in the input sequence.\n",
    "\n",
    "In the second part of this practical we will explore various different configurations for RNN models, including bi-directional and stacked (or deep) RNNs. We will also use Comet ML to record results and compare different models and hyperparameter configurations.\n",
    "\n",
    "The objectives of this practical are:\n",
    "1. Understand the key concepts for applying RNN to NLP tasks, specifically input representations and shape\n",
    "\n",
    "2. Consider different RNN configurations and output modes, including returning sequences, or only the last hidden state, bi-directional and stacked RNN\n",
    "\n",
    "3. Compare and contrast different hyperparameters and models, making use of ML platforms, like Comet ML, to record and present results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 LSTM for Sentiment Classification\n",
    "\n",
    "## 1.0 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim.downloader as gen\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the directory to the data folder\n",
    "data_dir = os.path.join('..', 'data', 'imdb')\n",
    "\n",
    "# Spacy needs to install the language model also\n",
    "# If you recieve an error, uncomment the following line and re-run the cell\n",
    "# !python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load and pre-process data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First load the full IMDB dataset and our smaller reviews set.\n",
    "\n",
    "2. Then we need to convert the 'positive' and 'negative' class labels to numerical values, 1 for positive and 0 for negative. Using the pandas `get_dummies` function creates two binary valued columns and then the `drop_first` parameter collapses these into a single column.\n",
    "\n",
    "3. The next cell plots the distribution of reveiew lengths in terms of number of tokens. This is important for RNN models because each token of an input review will be processed in turn, and so each input example will be a different length. Padding each review to the length of the longest in the data will increase processing time and may impact performance, so the distribution allows us to choose a suitable `max_seq_len` for the model to process. From the graph it looks like most reviews are less than 200 in length, so 100 to 200 should be a suitable `max_seq_len`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the imdb dataset\n",
    "imdb_data = pd.read_csv(os.path.join(data_dir, 'imdb_dataset.csv'))\n",
    "\n",
    "# Load your imdb reviews\n",
    "imdb_reviews = pd.read_csv(os.path.join(data_dir, 'imdb_reviews.csv'), index_col=0)\n",
    "# Just keep the review text and the sentiment columns\n",
    "imdb_reviews = imdb_reviews[['review', 'sentiment']]\n",
    "\n",
    "# Convert the sentiment to a binary value\n",
    "imdb_data['sentiment'] = pd.get_dummies(imdb_data['sentiment'], drop_first=True)\n",
    "imdb_reviews['sentiment'] = pd.get_dummies(imdb_reviews['sentiment'], drop_first=True)\n",
    "\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise the reviews\n",
    "tokenised_data = imdb_data['review'].apply(lambda x: [token.text for token in nlp.tokenizer(x)])\n",
    "\n",
    "# Plot the distribution of review lengths\n",
    "seq_lengths = [len(review) for review in tokenised_data]\n",
    "sns.displot(seq_lengths, kind='hist')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the vocabulary\n",
    "\n",
    "You should have also created a vocabulary from the larger IMDB dataset, so lets load that and set the vocabulary size accordingly. We will also add some special padding and unknown tokens for use later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the vocabulary file and store each word in a list\n",
    "with open(os.path.join(data_dir, 'imdb_vocab.txt'), 'r') as file:\n",
    "    imdb_vocab = file.read().splitlines() \n",
    "    \n",
    "# Set the vocab size\n",
    "vocab_size = 2000\n",
    "\n",
    "# Add the padding and unknown tokens to the vocabulary\n",
    "imdb_vocab.insert(0, '')\n",
    "imdb_vocab.insert(1, '[UNK]')\n",
    "imdb_vocab = imdb_vocab[:vocab_size]\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary size: \" + str(vocab_size))\n",
    "for i, word in enumerate(imdb_vocab[:50]):\n",
    "    print(f'({str(i)}, {word})', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and vectorise the text\n",
    "\n",
    "We will be using embedding representations as input to the RNN model, so each word in an input review needs to be mapped to its embedding vector. To do this we must first map each word to an integer according to its index within the vocabulary and then use that as a 'lookup' for the corresponding embedding. The result will be a 2D matrix/tensor of shape (`max_seq_len`, `embedding_dim`) for each input example. However, performing these steps manually beforhand results in a very large input tensor! So we will take advantage of the `TextVectorisation` layer and `Embedding` layer in Keras.\n",
    "\n",
    "1. We will split the data as normal, using the full IMDB data as training and validation and our reviews as a test set.\n",
    "\n",
    "2. Set up the text vectorisation layer with our vocabulary and specified `max_seq_len`. This layer takes a string input, tokenises each word, maps to integers and then outputs a sequence of the specified length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the maximum sequence legth\n",
    "max_seq_len = 200\n",
    "\n",
    "# Get the reviews\n",
    "X = imdb_data['review'].values\n",
    "print('Shape of X:', X.shape)\n",
    "print(X[:2])\n",
    "\n",
    "# Get the class labels\n",
    "y = imdb_data['sentiment'].values\n",
    "print('Shape of y:', y.shape)\n",
    "print(y[:2])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get the test data\n",
    "X_test = imdb_reviews['review'].values\n",
    "y_test = imdb_reviews['sentiment'].values\n",
    "\n",
    "# Create the text vectoriser\n",
    "vectorise_layer = layers.experimental.preprocessing.TextVectorization(\n",
    " max_tokens=vocab_size,\n",
    " standardize=None,\n",
    " split='whitespace',\n",
    " output_mode='int',\n",
    " output_sequence_length=max_seq_len,\n",
    " pad_to_max_tokens=False,\n",
    " vocabulary=imdb_vocab\n",
    " )\n",
    "\n",
    "# Adapt the vectoriser to the training data\n",
    "vectorise_layer.adapt(X_train)\n",
    "\n",
    "# Vocabulary size and content should be the same as above\n",
    "print(\"Vectoriser vocabulary size: \" + str(len(vectorise_layer.get_vocabulary())))\n",
    "print(vectorise_layer.get_vocabulary()[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an embedding matrix\n",
    "\n",
    "We need to create an embedding matrix of shape (`vocab_size`, `embedding_dim`). This will be the 'lookup' table for each word in an input sequence to be mapped to its embedding. We will use pre-trained [GloVe](https://nlp.stanford.edu/projects/glove/) embeddings as these should already contain information about the word relations from the millions of words they were trained on.\n",
    "\n",
    "1. Create a numpy embedding matrix by looping over the vocabulary and assigning each word an embedding. If the word is not present in the GloVe vectors we will generate a random embedding.\n",
    "\n",
    "2. Then create an embedding layer using the embedding matrix as the lookup. This layer takes a `max_seq_len` sequence of integers as input and maps each word to an embedding to produce a tensor of shape (`max_seq_len`, `embedding_dim`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the embedding dimension\n",
    "embedding_dim = 300\n",
    "\n",
    "# Load the word embeddings\n",
    "glove_vectors = gen.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "# Generate the embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for i, word in enumerate(imdb_vocab):\n",
    "    if word in glove_vectors:\n",
    "        embedding_matrix[i] = glove_vectors[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.uniform(low=-1.0, high=1.0, size=embedding_dim)\n",
    "\n",
    "print('Shape of embeddings:', embedding_matrix.shape)\n",
    "print(embedding_matrix[:5])\n",
    "\n",
    "# Create the embedding layer\n",
    "embedding_layer = layers.Embedding(\n",
    " input_dim=vocab_size,\n",
    " output_dim=embedding_dim,\n",
    " input_length=max_seq_len,\n",
    " embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    " trainable=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Build the RNN model\n",
    "\n",
    "The cell below creates a Keras functional model:\n",
    "- The input layer takes a batch of single strings\n",
    "\n",
    "- Vectorisation layer tokenises and maps each word to integers.\n",
    "\n",
    "- Embedding layer maps each sequence of words to embeddings\n",
    "\n",
    "- RNN processes each word in the sequence in turn. Here, because `return_sequences=False` we are taking the last hidden state as the encoded sequence representation.\n",
    "\n",
    "- Finally several feed forward layers perform classification.\n",
    "\n",
    "Study the model summary and make sure you are happy with the different operations and resulting tensor shapes as data passes through the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input layer takes in a string\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# Vectorise the inputs\n",
    "vectorised_inputs = vectorise_layer(inputs)\n",
    "\n",
    "# Embed the inputs\n",
    "embedded_inputs = embedding_layer(vectorised_inputs)\n",
    "\n",
    "# Recurrent layer\n",
    "x = layers.LSTM(128, return_sequences=False)(embedded_inputs)\n",
    "\n",
    "# Classification layers\n",
    "x = layers.Dropout(0.2)(x)\n",
    "x = layers.Dense(32, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Compile the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"LSTM\")\n",
    "model.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "results = model.fit(X_train, y_train, epochs=10, batch_size=128, validation_data=(X_val, y_val))\n",
    "print(\"Validation Accuracy:\", round(results.history[\"val_accuracy\"][-1], 3))\n",
    "\n",
    "# Predict class labels for test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = [0 if x < 0.5 else 1 for x in predictions] # Convert probabilities to binary\n",
    "print('Test Accuracy:', accuracy_score(y_test, predictions))\n",
    "\n",
    "# Print confusion matrix\n",
    "conf_matrix = ConfusionMatrixDisplay.from_predictions(y_test, predictions, display_labels=['negative', 'positive',], colorbar=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Experimenting with RNN\n",
    "\n",
    "## 2.0 Import libraries\n",
    "\n",
    "1. [Comet ML](https://www.comet.com/) - is an experiment and model tracking API which allows easy logging of model hyperparameters and experiment metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import comet_ml\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim.downloader as gen\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, ConfusionMatrixDisplay\n",
    "%matplotlib inline\n",
    "\n",
    "# Set the directory to the data folder\n",
    "data_dir = os.path.join('..', 'data', 'imdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>reviewers mentioned watching 1 oz episode hook...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically family little boy jake thinks zombie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei s love time money visually stunn...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  reviewers mentioned watching 1 oz episode hook...          1\n",
       "1  wonderful little production filming technique ...          1\n",
       "2  thought wonderful way spend time hot summer we...          1\n",
       "3  basically family little boy jake thinks zombie...          0\n",
       "4  petter mattei s love time money visually stunn...          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the imdb dataset\n",
    "imdb_data = pd.read_csv(os.path.join(data_dir, 'imdb_dataset.csv'))\n",
    "\n",
    "# Load your imdb reviews\n",
    "imdb_reviews = pd.read_csv(os.path.join(data_dir, 'imdb_reviews.csv'), index_col=0)\n",
    "# Just keep the review text and the sentiment columns\n",
    "imdb_reviews = imdb_reviews[['review', 'sentiment']]\n",
    "\n",
    "# Convert the sentiment to a binary value\n",
    "imdb_data['sentiment'] = pd.get_dummies(imdb_data['sentiment'], drop_first=True)\n",
    "imdb_reviews['sentiment'] = pd.get_dummies(imdb_reviews['sentiment'], drop_first=True)\n",
    "\n",
    "imdb_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 2000\n",
      "(0, ) (1, [UNK]) (2, movie) (3, film) (4, s) (5, like) (6, good) (7, time) (8, story) (9, bad) (10, people) (11, great) (12, way) (13, movies) (14, characters) (15, think) (16, watch) (17, character) (18, films) (19, seen) (20, love) (21, plot) (22, life) (23, acting) (24, best) (25, know) (26, little) (27, man) (28, better) (29, end) (30, scene) (31, scenes) (32, real) (33, thing) (34, watching) (35, actors) (36, director) (37, old) (38, years) (39, funny) (40, going) (41, work) (42, 10) (43, actually) (44, makes) (45, look) (46, find) (47, new) (48, lot) (49, want) "
     ]
    }
   ],
   "source": [
    "# Load the vocabulary file and store each word in a list\n",
    "with open(os.path.join(data_dir, 'imdb_vocab.txt'), 'r') as file:\n",
    "    imdb_vocab = file.read().splitlines() \n",
    "    \n",
    "# Set the vocab size\n",
    "vocab_size = 2000\n",
    "\n",
    "# Add the padding and unknown tokens to the vocabulary\n",
    "imdb_vocab.insert(0, '')\n",
    "imdb_vocab.insert(1, '[UNK]')\n",
    "imdb_vocab = imdb_vocab[:vocab_size]\n",
    "\n",
    "# Print the vocabulary\n",
    "print(\"Vocabulary size: \" + str(vocab_size))\n",
    "for i, word in enumerate(imdb_vocab[:50]):\n",
    "    print(f'({str(i)}, {word})', end=' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process and vectorise the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (50000,)\n",
      "['reviewers mentioned watching 1 oz episode hooked right exactly happened thing struck oz brutality unflinching scenes violence set right word trust faint hearted timid pulls punches regards drugs sex violence hardcore classic use word called oz nickname given oswald maximum security state penitentary focuses mainly emerald city experimental section prison cells glass fronts face inwards privacy high agenda city home aryans muslims gangstas latinos christians italians irish scuffles death stares dodgy dealings shady agreements far away main appeal fact goes shows dare forget pretty pictures painted mainstream audiences forget charm forget romance oz mess episode saw struck nasty surreal ready watched developed taste oz got accustomed high levels graphic violence violence injustice crooked guards sold nickel inmates kill order away mannered middle class inmates turned prison bitches lack street skills prison experience watching oz comfortable uncomfortable viewing touch darker'\n",
      " 'wonderful little production filming technique unassuming old time bbc fashion gives comforting discomforting sense realism entire piece actors extremely chosen michael sheen got polari voices pat truly seamless editing guided references williams diary entries worth watching terrificly written performed piece masterful production great master s comedy life realism comes home little things fantasy guard use traditional dream techniques remains solid disappears plays knowledge senses particularly scenes concerning orton halliwell sets particularly flat halliwell s murals decorating surface terribly']\n",
      "Shape of y: (50000,)\n",
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "# Set the maximum sequence legth\n",
    "max_seq_len = 200\n",
    "\n",
    "# Get the reviews\n",
    "X = imdb_data['review'].values\n",
    "print('Shape of X:', X.shape)\n",
    "print(X[:2])\n",
    "\n",
    "# Get the class labels\n",
    "y = imdb_data['sentiment'].values\n",
    "print('Shape of y:', y.shape)\n",
    "print(y[:2])\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Get the test data\n",
    "X_test = imdb_reviews['review'].values\n",
    "y_test = imdb_reviews['sentiment'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of embeddings: (2000, 300)\n",
      "[[-0.14515546 -0.39247585 -0.25915112 ...  0.4540832  -0.04217864\n",
      "   0.66845111]\n",
      " [-0.8630558  -0.4488386   0.73237626 ...  0.46465078  0.79680394\n",
      "   0.45015108]\n",
      " [-0.138      -0.12203     0.0054643  ...  0.19934     0.057473\n",
      "  -0.023767  ]\n",
      " [-0.030351   -0.17344999 -0.097576   ...  0.26174     0.083567\n",
      "  -0.19064   ]\n",
      " [ 0.31000999  0.046907   -0.31283    ... -0.74206001 -0.41123\n",
      "   0.73799002]]\n"
     ]
    }
   ],
   "source": [
    "# Set the embedding dimension\n",
    "embedding_dim = 300\n",
    "\n",
    "# Load the word embeddings\n",
    "glove_vectors = gen.load(\"glove-wiki-gigaword-300\")\n",
    "\n",
    "# Generate the embedding matrix\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for i, word in enumerate(imdb_vocab):\n",
    "    if word in glove_vectors:\n",
    "        embedding_matrix[i] = glove_vectors[word]\n",
    "    else:\n",
    "        embedding_matrix[i] = np.random.uniform(low=-1.0, high=1.0, size=embedding_dim)\n",
    "\n",
    "print('Shape of embeddings:', embedding_matrix.shape)\n",
    "print(embedding_matrix[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Set up the Comet experiment and build the model\n",
    "\n",
    "Here we will train a similar LSTM model but we will use Comet ML to log the training metrics and model hyperparameters. This will allow us to compare the impact of different models and different hyperparamters on the sentiment classification task. Comet allows you to track models, experiments and also produce graphs for comparison.\n",
    "\n",
    "1. You will need a Comet ML account and  optionally to have created a project. Once you have an account you will need your API key. A guide can be found [here](https://www.comet.com/docs/v2/guides/getting-started/quickstart/).\n",
    "\n",
    "2. The first two lines create a comet experiment object that will be used to track the training and log metrics/parameters. You need the API key, to (optionally) provide a project name and set the workspace to your username. Then we can also provide a name for this experiment.\n",
    "\n",
    "3. Next some experiment and model hyperparameters are set within a dictionary. These can be any relevant values but for now we have one pre-processing parameter (`max_seq_len`), several model hyperparameters and some training parameters (`batch_size` and `epochs`).\n",
    "\n",
    "4. Finally, build the model as before, except use the parameters defined within the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET WARNING: As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "COMET INFO: Experiment is live on comet.com https://www.comet.com/nathanduran/general/f4de6a0fc25442feb384d2522a36f59c\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"LSTM\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         [(None, 1)]               0         \n",
      "_________________________________________________________________\n",
      "text_vectorization_2 (TextVe (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 200, 300)          600000    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 128)               219648    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 823,809\n",
      "Trainable params: 223,809\n",
      "Non-trainable params: 600,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create the comet ml experiment\n",
    "experiment = comet_ml.Experiment(workspace=\"nathanduran\", log_env_gpu=False)\n",
    "experiment.set_name('lstm')\n",
    "\n",
    "# Set and log the hyperparameters\n",
    "params = {'max_seq_len': 100,\n",
    "          'lstm_units': 128,\n",
    "          'dropout': 0.2,\n",
    "          'dense_units': 32,\n",
    "          'optimizer': 'adam',\n",
    "          'learning_rate': 0.001,\n",
    "          'batch_size': 128,\n",
    "          'epochs': 10,}\n",
    "\n",
    "experiment.log_parameters(params)\n",
    "\n",
    "# Create the text vectoriser\n",
    "vectorise_layer = layers.experimental.preprocessing.TextVectorization(\n",
    " max_tokens=vocab_size,\n",
    " standardize=None,\n",
    " split='whitespace',\n",
    " output_mode='int',\n",
    " output_sequence_length=max_seq_len,\n",
    " pad_to_max_tokens=False,\n",
    " vocabulary=imdb_vocab\n",
    " )\n",
    "\n",
    "# Adapt the vectoriser to the training data\n",
    "vectorise_layer.adapt(X_train)\n",
    "\n",
    "# Create the embedding layer\n",
    "embedding_layer=layers.Embedding(\n",
    " input_dim=vocab_size,\n",
    " output_dim=embedding_dim,\n",
    " input_length=max_seq_len,\n",
    " embeddings_initializer=tf.keras.initializers.Constant(embedding_matrix),\n",
    " trainable=False)\n",
    "\n",
    "# Input layer takes in a string\n",
    "inputs = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "# Vectorise the inputs\n",
    "vectorised_inputs = vectorise_layer(inputs)\n",
    "\n",
    "# Embed the inputs\n",
    "embedded_inputs = embedding_layer(vectorised_inputs)\n",
    "\n",
    "# Recurrent layer\n",
    "x = layers.LSTM(params['lstm_units'], return_sequences=False)(embedded_inputs)\n",
    "\n",
    "# Classification layers\n",
    "x = layers.Dropout(params['dropout'])(x)\n",
    "x = layers.Dense(params['dense_units'], activation=\"relu\")(x)\n",
    "x = layers.Dropout(params['dropout'])(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Compile the model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"LSTM\")\n",
    "\n",
    "# Create the optimiser\n",
    "optimiser = tf.keras.optimizers.Adam(lr=params['learning_rate'])\n",
    "\n",
    "model.compile(optimizer=optimiser, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate the model\n",
    "\n",
    "Train the model as before, but use `model.evaluate()` to get a test set accuracy and log that with Comet. We will also log the confusion matrix so that it can be viewed in the experiment on Comet.\n",
    "\n",
    "Once training and evaluation is complete the experiment should be visible on Comet ML. You can create graphs (Panels) to display results accross multiple experiments. You can also view the results of each individual experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "313/313 [==============================] - 145s 462ms/step - loss: 0.6955 - accuracy: 0.5042 - val_loss: 0.6926 - val_accuracy: 0.5020\n",
      "Epoch 2/10\n",
      "313/313 [==============================] - 219s 703ms/step - loss: 0.6920 - accuracy: 0.5118 - val_loss: 0.6917 - val_accuracy: 0.5076\n",
      "Epoch 3/10\n",
      "313/313 [==============================] - 87s 275ms/step - loss: 0.6924 - accuracy: 0.5053 - val_loss: 0.6922 - val_accuracy: 0.5061\n",
      "Epoch 4/10\n",
      "313/313 [==============================] - 60s 193ms/step - loss: 0.6911 - accuracy: 0.5134 - val_loss: 0.6873 - val_accuracy: 0.5151\n",
      "Epoch 5/10\n",
      "313/313 [==============================] - 54s 172ms/step - loss: 0.6736 - accuracy: 0.5455 - val_loss: 0.4957 - val_accuracy: 0.7666\n",
      "Epoch 6/10\n",
      "313/313 [==============================] - 43s 139ms/step - loss: 0.4859 - accuracy: 0.7763 - val_loss: 0.3976 - val_accuracy: 0.8205\n",
      "Epoch 7/10\n",
      "313/313 [==============================] - 65s 208ms/step - loss: 0.3803 - accuracy: 0.8369 - val_loss: 0.3573 - val_accuracy: 0.8417\n",
      "Epoch 8/10\n",
      "313/313 [==============================] - 42s 133ms/step - loss: 0.3242 - accuracy: 0.8653 - val_loss: 0.3446 - val_accuracy: 0.8493\n",
      "Epoch 9/10\n",
      "313/313 [==============================] - 53s 168ms/step - loss: 0.2832 - accuracy: 0.8850 - val_loss: 0.3392 - val_accuracy: 0.8494\n",
      "Epoch 10/10\n",
      "313/313 [==============================] - 55s 175ms/step - loss: 0.2433 - accuracy: 0.9035 - val_loss: 0.3842 - val_accuracy: 0.8437\n",
      "Validation Accuracy: 0.844\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.6431 - accuracy: 0.7200\n",
      "Test Accuracy: 0.72\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Comet.ml Experiment Summary\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : https://www.comet.com/nathanduran/general/f4de6a0fc25442feb384d2522a36f59c\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     accuracy [10]                : (0.5045750141143799, 0.9006749987602234)\n",
      "COMET INFO:     batch_accuracy [320]         : (0.421875, 0.9112215638160706)\n",
      "COMET INFO:     batch_loss [320]             : (0.23388151824474335, 0.7068315744400024)\n",
      "COMET INFO:     epoch_duration [10]          : (41.54600000008941, 219.34400000004098)\n",
      "COMET INFO:     loss [10]                    : (0.24950693547725677, 0.6938596963882446)\n",
      "COMET INFO:     test_accuracy                : 0.7200000286102295\n",
      "COMET INFO:     val_accuracy [10]            : (0.5019999742507935, 0.849399983882904)\n",
      "COMET INFO:     val_loss [10]                : (0.339186429977417, 0.6925872564315796)\n",
      "COMET INFO:     validate_batch_accuracy [80] : (0.4765625, 0.8671875)\n",
      "COMET INFO:     validate_batch_loss [80]     : (0.27697989344596863, 0.6983223557472229)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Name             : lstm\n",
      "COMET INFO:     trainable_params : 823809\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     Adam_amsgrad       : False\n",
      "COMET INFO:     Adam_beta_1        : 0.9\n",
      "COMET INFO:     Adam_beta_2        : 0.999\n",
      "COMET INFO:     Adam_decay         : 0.0\n",
      "COMET INFO:     Adam_epsilon       : 1e-07\n",
      "COMET INFO:     Adam_learning_rate : 0.001\n",
      "COMET INFO:     Adam_name          : Adam\n",
      "COMET INFO:     Optimizer          : Adam\n",
      "COMET INFO:     batch_size         : 128\n",
      "COMET INFO:     dense_units        : 32\n",
      "COMET INFO:     dropout            : 0.2\n",
      "COMET INFO:     epochs             : 10\n",
      "COMET INFO:     learning_rate      : 0.001\n",
      "COMET INFO:     lstm_units         : 128\n",
      "COMET INFO:     max_seq_len        : 100\n",
      "COMET INFO:     optimizer          : adam\n",
      "COMET INFO:     steps              : 313\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     confusion-matrix         : 1\n",
      "COMET INFO:     environment details      : 1\n",
      "COMET INFO:     filename                 : 1\n",
      "COMET INFO:     git metadata             : 1\n",
      "COMET INFO:     git-patch (uncompressed) : 1 (53.18 KB)\n",
      "COMET INFO:     installed packages       : 1\n",
      "COMET INFO:     model graph              : 1\n",
      "COMET INFO:     notebook                 : 1\n",
      "COMET INFO:     source_code              : 1\n",
      "COMET INFO: ---------------------------\n",
      "COMET INFO: Uploading 1 metrics, params and output messages\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "results = model.fit(X_train, y_train, epochs=params['epochs'], batch_size=params['batch_size'], validation_data=(X_val, y_val))\n",
    "print(\"Validation Accuracy:\", round(results.history[\"val_accuracy\"][-1], 3))\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print('Test Accuracy:', round(test_acc, 3))\n",
    "experiment.log_metric(\"test_accuracy\", test_acc)\n",
    "\n",
    "# Predict class labels for test set\n",
    "predictions = model.predict(X_test)\n",
    "predictions = [0 if x < 0.5 else 1 for x in predictions] # Convert probabilities to binary\n",
    "\n",
    "# Print/log confusion matrix\n",
    "conf_matrix = ConfusionMatrixDisplay.from_predictions(y_test, predictions, display_labels=['negative', 'positive'], colorbar=False)\n",
    "plt.show()\n",
    "experiment.log_confusion_matrix(y_test, predictions, labels=['negative', 'positive'])\n",
    "\n",
    "# End the experiment\n",
    "experiment.end()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Exercise: Comet experiments and different parameters\n",
    "\n",
    "So far we have only recorded a single experiment on Comet which uses the parameters we have already defined. Try exploring some configurations parameters to see if you can improve upon the accuracy achieved so far.\n",
    "\n",
    "1. You should be able to provide access to a Comet project for all members of the group, so that everyone can log experiments. Individually you could each explore the impact of different experiment parameters and then use Comet to compare results e.g. one person explores sequence lengths, another model hyperparameters, optimisers and learning rates and so on.\n",
    "\n",
    "2. Experiment with the panels and other visualisation options that are available on Comet to display your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Exercise: RNN architectures\n",
    "\n",
    "As discussed in the lecture, there are several different architectural variations of RNN models, such as Bi-directional and stacked layers. Try experimenting with a few different architectures and evaluate the impact on classification accuracy.\n",
    "\n",
    "1. A good first step might be to take output from *each* timestep (`return_sequences=True`), rather than the last only. You will need an additional pooling layer to perform dimensionality reduction on the RNN outputs so that they are compatible with the later classification layers.\n",
    "\n",
    "2. Use the Keras [Bi-directional layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Bidirectional) to convert the uni-directional LSTM into bi-directional.\n",
    "\n",
    "3. Try stacking two LSTM layers. The first will need to output sequences, which become the inputs for the second layer. Then you can either take the last hidden state as output (as we did before), or pool the sequence of outputs, as in step 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "eda59365f9d652723e3bcf67739b9100ac1f6ab6ddfa121c8653940903b971a2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
