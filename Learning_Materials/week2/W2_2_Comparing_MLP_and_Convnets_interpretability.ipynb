{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A visual comparison of MLPs and Convolutional Networks \n",
    "\n",
    "### very loosely based on example visualisation code  from a Kaggle example  and \"towards data science\"\n",
    "\n",
    "This notebook contrasts how simple  [Multi-layer Perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron) (MLP) Neural Networks and Convolutional Neural Networks recognize hand-written digits from the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) data set. \n",
    "\n",
    "* A two-layer MLP does a decent job at recognizing hand-written digits.\n",
    "\n",
    "* A very simple convolutional model does even better (though still not close to the 'state of the art')\n",
    "* There is considerable difference in the *interpretability* of the features\n",
    "\n",
    "Author @Jim Smith james.smith@uwe.ac.uk 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import socket\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Activation, Dropout\n",
    "from keras.layers import Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical, plot_model\n",
    "from keras.datasets import mnist\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# First load and transform the mnist data\n",
    "Just to speed thing up we will only take 20% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/mnistmnist_train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../datasets/mnist\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m fraction\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m \u001b[38;5;66;03m#i.e use 1/5th of data\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmnist_train.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m x_train \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[:data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m:\u001b[38;5;241m785\u001b[39m]\n\u001b[1;32m     10\u001b[0m x_train \u001b[38;5;241m=\u001b[39m x_train\u001b[38;5;241m.\u001b[39mvalues\u001b[38;5;241m.\u001b[39mreshape(data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m784\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/py10/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py10/lib/python3.10/site-packages/pandas/util/_decorators.py:317\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    312\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    313\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    314\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    315\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(inspect\u001b[38;5;241m.\u001b[39mcurrentframe()),\n\u001b[1;32m    316\u001b[0m     )\n\u001b[0;32m--> 317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniforge3/envs/py10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/py10/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1729\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1727\u001b[0m     is_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1728\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1729\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1730\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1731\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1732\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1733\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1734\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1735\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1740\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniforge3/envs/py10/lib/python3.10/site-packages/pandas/io/common.py:857\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    856\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 857\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    864\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    865\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    866\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/mnistmnist_train.csv'"
     ]
    }
   ],
   "source": [
    "if (socket.gethostname()=='csctcloud'): #on csctcloud\n",
    "    path=\"/home/common/datasets/mninst\"\n",
    "else: #machie specific- this is for jim's development\n",
    "    path = \"../datasets/mnist\"\n",
    "\n",
    "fraction= 5 #i.e use 1/5th of data\n",
    "\n",
    "data = pd.read_csv(path +'mnist_train.csv')\n",
    "x_train = data.iloc[:data.shape[0],1:785]\n",
    "x_train = x_train.values.reshape(data.shape[0], 784)\n",
    "x_train = x_train[:x_train.shape[0]//5,:]\n",
    "y_train = data.iloc[:data.shape[0]//fraction,0]\n",
    "\n",
    "testdata = pd.read_csv(path + 'mnist_test.csv')\n",
    "x_test = testdata.iloc[:testdata.shape[0],1:785]\n",
    "x_test = x_test.values.reshape(testdata.shape[0], 784)\n",
    "x_test = x_test[:x_test.shape[0]//5,:]\n",
    "y_test = testdata.iloc[:testdata.shape[0]//fraction,0]\n",
    "\n",
    "# compute the number of labels\n",
    "num_labels = len(np.unique(y_train))\n",
    "\n",
    "# convert to one-hot vector\n",
    "y_train = to_categorical(y_train)\n",
    "y_test = to_categorical(y_test)\n",
    "\n",
    "# normalize\n",
    "x_train = x_train.astype('float32') / 255\n",
    "x_test = x_test.astype('float32') / 255\n",
    "\n",
    "#make version in 2d format for conv nets\n",
    "X_train_img = x_train.reshape(x_train.shape[0],28,28,1)\n",
    "X_test_img = x_test.reshape(x_test.shape[0],28,28,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visalise some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "f, axes = plt.subplots(2, 10, sharey=True,figsize=(20,5))\n",
    "for i,ax in enumerate(axes.flat):\n",
    "    ax.axis('off')\n",
    "    ax.imshow(X_test_img[i,:,:,0],cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Part 1: Specifying and Training a simple Neural  Network (MLP)\n",
    "\n",
    "We use a simple two-layer MLP with sigmoid activation \n",
    "<img align=\"right\" src=\"images/simple_MLP_for_Mnist.png\" alt=\"Architecture of simple MLP, only 28 inputs shown\" width=\"400\"/>\n",
    " \n",
    "**Note that. tyhis image only shows the first 28 of the (28 x 28=) 784 actual inputs** \n",
    " \n",
    "* In the first hidden layer, each neuron takes every pixel value as input parameter. \n",
    "* Every neuron in the second hidden layer then takes all the outputs of the first layer (after activation using sigmoid) as input parameters. \n",
    "* After applying a softmax activation, these results form the final output layer. \n",
    "\n",
    "The optimizer then derives linear weights in such a way as to minimize the loss function (in this case the *categorical crossentropy*) and thus maximizing the accuracy of the classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# network parameters\n",
    "input_size = x_train.shape[1]\n",
    "batch_size = 64\n",
    "activation = 'sigmoid'\n",
    "# this model is a 3-layer MLP with sigmoid activation each layer\n",
    "model = Sequential()\n",
    "model.add(Dense(25, input_dim=input_size, activation='sigmoid'))\n",
    "model.add(Dense(25,activation='sigmoid'))\n",
    "model.add(Dense(num_labels,activation='softmax'))\n",
    "model.summary()\n",
    "\n",
    "# loss function for one-hot vector using adam optimizer\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now train the model on our subset of the  MNIST training data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# train the network\n",
    "model.fit(x_train, y_train, epochs=20, batch_size=batch_size)\n",
    "\n",
    "# validate the model on test dataset to determine generalization\n",
    "loss, acc = model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This model implementation reaches an accuracy of ~96% if we give it the whole training set, which is remarkable, but better results are possible using, e.g., a [Convolutional Neural Network](https://en.wikipedia.org/wiki/Convolutional_neural_network). We use the simpler MLP to make the interpretation of the visualization results easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Extract Output and Group Information\n",
    "\n",
    "We extract the outputs generated by each individual neuron and for each frame in the MNIST training sample and store them on a per-layer basis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "get_layer_output = K.function( inputs = model.layers[0].input,\n",
    "                              outputs=[model.layers[0].output, model.layers[1].output, model.layers[2].output])\n",
    "\n",
    "layer1_output, layer2_output, layer3_output = get_layer_output([x_train])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, we extract and store the indices of frames showing the same digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_ids = [np.arange(len(y_train))[y_train[:,i] == 1] for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Visualization of  Individual Frames\n",
    "\n",
    "In this visualization, we focus on individual training data (i.e., individual frames with hand-written digits).\n",
    "\n",
    "The following panel shows from left to right \n",
    "* the original 28x28 pixel frame depicting a hand-written figure,\n",
    "* the output values of all neurons of the first hidden layer,\n",
    "* the output values of all neurons of the second hidden layer, and\n",
    "* the one-hot encoded output layer indicating the model classification result.\n",
    "\n",
    "Note that in those plots showing network layers, each pixel stands for the output of a single neuron. This output is based on the input parameters passed on from the previous layer, the trained weights for each neuron, and the activation function used in this layer. Dark blue pixels stand for low output values, while yellow pixels stand for high output values. The pixels have been arranged in two dimensions to save space; just think of these layers in linear arrangements to stay in the typical picture of layers in a network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%matplotlib inline\n",
    "\n",
    "# digit to be plotted\n",
    "digit = 4\n",
    "\n",
    "# indices of frames to be plotted for this digit\n",
    "n = range(50)\n",
    "\n",
    "# initialize plots\n",
    "f, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize=(15,4))\n",
    "\n",
    "# prepare plots\n",
    "ax1.set_title('Input Layer', fontsize=16)\n",
    "ax1.axes.get_xaxis().set_visible(False)\n",
    "ax1.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax2.set_title('Hidden Layer 1', fontsize=16)\n",
    "ax2.axes.get_xaxis().set_visible(False)\n",
    "ax2.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "ax3.set_title('Hidden Layer 2', fontsize=16)\n",
    "ax3.axes.get_xaxis().set_visible(False)\n",
    "ax3.axes.get_yaxis().set_visible(False)\n",
    "    \n",
    "ax4.set_title('Output Layer', fontsize=16)\n",
    "ax4.axes.get_xaxis().set_visible(False)\n",
    "ax4.axes.get_yaxis().set_visible(False)   \n",
    "\n",
    "# add numbers to the output layer plot to indicate label\n",
    "for i in range(3):\n",
    "    for j in range(4):\n",
    "        text = ax4.text(j, i, [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, '', '']][i][j],\n",
    "                        ha=\"center\", va=\"center\", color=\"w\", fontsize=16)    \n",
    "        \n",
    "def animate(id):\n",
    "    # plot elements that are changed in the animation\n",
    "    digit_plot = ax1.imshow(x_train[train_ids[digit][id]].reshape((28,28)), animated=True)\n",
    "    layer1_plot = ax2.imshow(layer1_output[train_ids[digit][id]].reshape((5,5)), animated=True)\n",
    "    layer2_plot = ax3.imshow(layer2_output[train_ids[digit][id]].reshape((5,5)), animated=True)\n",
    "    output_plot = ax4.imshow(np.append(layer3_output[train_ids[digit][id]], \n",
    "                                       [np.nan, np.nan]).reshape((3,4)), animated=True)\n",
    "    return digit_plot, layer1_plot, layer2_plot, output_plot,\n",
    "\n",
    "# define animation\n",
    "ani = matplotlib.animation.FuncAnimation(f, animate, frames=n, interval=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scrolling through the animation, it becomes clear that in most cases the same subset of neurons fires, while other neurons remain turned off.\n",
    "\n",
    "This is much more obvious in the second hidden layer than in the first hidden layer.\n",
    "\n",
    "It can be interpreted as the first layer pre-processesing the pixel data,  \n",
    "while the second layer deals with pattern recognition.  \n",
    "Note that in most cases the recognition of the digit shown is unambiguous; ambiguity only occurs in somewhat pathologic cases.\n",
    "\n",
    "You can change the digit shown by changing the `digit` value in the code block above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Conclusions\n",
    "\n",
    "* Despite variations in the shapes of hand-written digits, the same groups of neurons is involved in the identification of the same digits.\n",
    "* Similarities in the shapes of digits translate into similarities in the groups of neurons that are involved in their identification in the first hidden layer, but not so much in the second hidden layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# now with a conv-net\n",
    "adapted from notebook at \n",
    "https://towardsdatascience.com/visualizing-intermediate-activations-of-a-cnn-trained-on-the-mnist-dataset-2c34426416c8\n",
    "\n",
    "my version uses a **much** simpler convolutional structure and changes some of the viusualisations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cmodel(c_width1:int,c_width2:int, fc_width:int):\n",
    "    \n",
    "    inputs = Input(shape=(28,28,1))  \n",
    "    x = Conv2D(c_width1,kernel_size=(5,5),padding='same',activation=\"relu\",name=f\"conv1_{c_width1}x5x5\")(inputs) \n",
    "    x = MaxPooling2D(pool_size=(2, 2),name=\"maxpool_2x2\")(x) # 8 x (14 x 14)  \n",
    "    x = Conv2D(c_width2, (3, 3), padding='same',activation='relu',name=f\"conv2_{c_width2}x3x3\")(x)\n",
    "    x = MaxPooling2D(pool_size=(2, 2),name=\"maxpool2_2x2\")(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(fc_width, activation='relu',name=f\"dense1_{fc_width}\")(x)\n",
    "    \n",
    "    output = Dense(num_classes,activation=\"softmax\",name=\"dense_10\")(x)\n",
    "    \n",
    "    model = Model(inputs,output)\n",
    "    model.summary()\n",
    "    model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='adam', \n",
    "              metrics=['accuracy'])\n",
    "    \n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_train2, X_test2, Y_train2, Y_test2 = train_test_split(X_train_img,y_train,test_size=0.20, random_state=42)\n",
    "  \n",
    "batch_size=256\n",
    "num_classes = 10\n",
    "epochs = 20\n",
    "c_width1=8\n",
    "c_width2 =8\n",
    "fc_width=20\n",
    "conv_model = cmodel(c_width1=c_width1,c_width2=c_width2,fc_width=fc_width)  \n",
    "\n",
    "history = conv_model.fit(X_train2,Y_train2,\n",
    "         epochs=epochs,\n",
    "         batch_size=batch_size,\n",
    "         validation_data=(X_test2,Y_test2))\n",
    "\n",
    "\n",
    "loss, acc = conv_model.evaluate(X_test2, Y_test2, batch_size=batch_size)\n",
    "print(\"\\nTest accuracy: %.1f%%\" % (100.0 * acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# A very simple Convolutional Architecture\n",
    "\n",
    "<img align=\"centre\" src=\"images/convnet_fig.png\" alt=\"Architecture of simple 2-layer conv net architecture\" width=\"600\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First let's look at the learned weights for each filter in our trained model\n",
    "Because as we saw, this is what they will respond to in the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs =plt.subplots(2,max(c_width1, c_width2),figsize=(8, 4))\n",
    "\n",
    "axs[0][0].set_ylabel('Conv layer 1', fontsize=10)\n",
    "for col in range (c_width1):\n",
    "    weights = conv_model.get_weights()[0][:,:,0,col]\n",
    "    axs[0][col].imshow(weights, cmap='RdYlGn',vmin=-0.5,vmax=0.5)\n",
    "\n",
    "axs[1][0].set_ylabel('Conv. layer 2', fontsize=10)\n",
    "for col in range (c_width1):\n",
    "    weights=  conv_model.get_weights()[2][:,:,0,col]\n",
    "    axs[1][col].imshow(weights, cmap='RdYlGn',vmin=-0.5,vmax=0.5)\n",
    "\n",
    "    \n",
    "#turn off annoying tick makrs\n",
    "for axrow in axs:\n",
    "    for axcol in axrow:\n",
    "        axcol.set_xticks([])\n",
    "        axcol.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now we can look at the layer activations for this different architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "layer_outputs = [layer.output for layer in conv_model.layers[1:7]]\n",
    "activation_model = Model(inputs=conv_model.input,outputs=layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#get list of all training images for each digit\n",
    "digit=4\n",
    "img_ids = [np.arange(len(Y_train2))[Y_train2[:,i] == 1] for i in range(10)]\n",
    "\n",
    "imgs =X_train2[img_ids[digit]]\n",
    "\n",
    "activations = activation_model.predict(imgs[0].reshape(1,28,28,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get layer names and descriptions\n",
    "\n",
    "cols_in_fig=0\n",
    "layer_names = []\n",
    "layer_sizes= []\n",
    "for layer in range(len(activations)):\n",
    "    layer_name= conv_model.layers[layer].name\n",
    "    print(f' layer {layer}  {layer_name} shape {activations[layer].shape}')\n",
    "    if len(activations[layer].shape)==4:\n",
    "        cols_in_fig+=1\n",
    "        layer_names.append(layer_name) \n",
    "        layer_sizes.append(activations[layer].shape)\n",
    "print(layer_names,layer_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#Make plot to animate\n",
    "columns= len(layer_names)\n",
    "rows=max(c_width1,c_width2)\n",
    "fig,axs =plt.subplots(rows,columns,figsize=(5, 8))\n",
    "\n",
    "for col in range(columns):\n",
    "    axs[0][col].set_title(layer_names[col],fontsize=10)\n",
    "\n",
    "col0 = axs[c_width1//2][0].imshow(imgs[0,:,:,0],cmap=\"gray\")\n",
    "\n",
    "#go through layers adding empty images\n",
    "for row in range(c_width1):#layer_sizes[1][3]):\n",
    "    the_output= axs[row][1].imshow( np.zeros((layer_sizes[1][1],layer_sizes[1][2])),cmap='RdYlGn')\n",
    "\n",
    "for row in range(c_width1):#layer_sizes[2][3]):\n",
    "    the_output= axs[row][2].imshow( np.zeros((layer_sizes[2][1],layer_sizes[2][2])),cmap='RdYlGn')\n",
    "\n",
    "\n",
    "#col3 = []\n",
    "for row in range(c_width2):#layer_sizes[3][3]):\n",
    "    the_output= axs[row][3].imshow( np.zeros((layer_sizes[3][1],layer_sizes[3][2])),cmap='RdYlGn')\n",
    "\n",
    "for axrow in axs:\n",
    "    for axcol in axrow:\n",
    "        axcol.set_xticks([])\n",
    "        axcol.set_yticks([])\n",
    "\n",
    "plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def animate(id):\n",
    "    \n",
    "    activations = activation_model.predict(imgs[id].reshape(1,28,28,1),verbose=1)\n",
    "\n",
    "    col0.set_data(imgs[id])\n",
    "    \n",
    "    for row in range( c_width1):\n",
    "        col1_img=np.zeros((28,28))\n",
    "        col2_img=np.zeros((14,14))\n",
    "  \n",
    "        \n",
    "        for arow in range(28):\n",
    "            for acol in range(28):\n",
    "                col1_img[arow][acol]= activations[0][0][arow][acol][row]\n",
    "        axs[row][1].imshow(col1_img,cmap='RdYlGn')\n",
    "        \n",
    "        for brow in range (14):\n",
    "            for bcol in range(14):\n",
    "                col2_img[brow][bcol]=activations[1][0][brow][bcol][row]\n",
    "        axs[row][2].imshow(col2_img,cmap='RdYlGn')\n",
    "\n",
    "    for row in range( c_width2):\n",
    "        col3_img=np.zeros((14,14))\n",
    "        for crow in range (14):\n",
    "            for ccol in range(14):\n",
    "                col3_img[crow][ccol]=activations[2][0][crow][ccol][row]\n",
    "        axs[row][3].imshow(col3_img,cmap='RdYlGn')\n",
    "    \n",
    "    return col0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "num_to_show=50\n",
    "ani =   matplotlib.animation.FuncAnimation(fig, animate, frames=num_to_show, interval=50)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ani"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "- which were more interpretable?\n",
    "- size of networks\n",
    "- avoiding risk of overfitting  \n",
    "  **what else could have been done to avoid over-fitting?**\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
