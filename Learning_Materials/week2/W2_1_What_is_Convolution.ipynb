{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec043f38-35f1-4d1a-9427-e1644ab55987",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Convolution_Example.ipynb\n",
    "# Simple Notebook to illustrate the idea of how image processing uses kernels to process imgaes\n",
    "# uses convolution.py from the Convolution-from-scratch repository by Nikita Detkov\n",
    "# Copyright Jim Smith james.smith@uwe.ac.uk 2023\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation \n",
    "from matplotlib.patches import Rectangle\n",
    "from convolution import conv2d\n",
    "\n",
    "plt.rcParams[\"animation.html\"] = \"jshtml\"\n",
    "plt.rcParams['figure.dpi'] = 150  \n",
    "plt.rcParams['animation.embed_limit']= 201004050\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83eba0b0-e182-4e78-9cd1-6cbbb0fb1d4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolution from the perspective of image processing\n",
    "For decades before the invention of 'neocognitrons' (nowadays better known as Convolutional Neural Networks,\n",
    "Image Processing Experts relied on the concepts of transforming images by moving a small 'kernel' across the image.\n",
    "\n",
    "The basic idea is to transform the image by using simple local processing which you apply at each different point in the image.\n",
    "\n",
    "To illustrate this process we will start by using a hand-drawn number (so we can process it quickly without code optimisations) and a simple vertical edge detector called the Prewitt Filter. (Technically the Prewitt uses vertical and horizontal versions then combines them).\n",
    "\n",
    "We'll use the terms *kernel* and *filter* pretty much interchangeably"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650fe6fc-3d41-4a37-a512-5919b67e3c44",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Get a simple hand-drawn image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f191e3-0c60-46f6-8d19-907722eefdfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "matrix = np.asarray([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,67,232,39,0,0],\n",
    "                     [0,0,0,0,62,81,0,0,0,0,0,0,0,0,0,0,0,0,0,0,120,180,39,0,0],\n",
    "                     [0,0,0,0,126,163,0,0,0,0,0,0,0,0,0,0,0,0,0,2,153,210,40,0,0],\n",
    "                     [0,0,0,0,220,163,0,0,0,0,0,0,0,0,0,0,0,0,0,27,254,162,0,0,0],\n",
    "                     [0,0,0,0,222,163,0,0,0,0,0,0,0,0,0,0,0,0,0,183,254,125,0,0,0],\n",
    "                     [0,0,0,0,46,245,0,0,0,0,0,0,0,0,0,0,0,0,0,198,254,56,0,0,0],\n",
    "                     [0,0,0,0,120,254,0,0,0,0,0,0,0,0,0,0,0,0,23,231,254,29,0,0,0],\n",
    "                     [0,0,0,0,159,254,0,0,0,0,0,0,0,0,0,0,0,0,163,254,216,16,0,0,0],\n",
    "                     [0,0,0,0,159,254,0,0,0,0,0,0,0,0,0,14,86,178,248,254,91,0,0,0,0],\n",
    "                     [0,0,0,0,159,254,35,0,0,47,49,116,144,150,241,243,234,179,241,252,40,0,0,0,0],\n",
    "                     [0,0,0,0,150,253,237,207,207,207,253,254,250,240,198,143,91,28,233,250,0,0,0,0,0],\n",
    "                     [0,0,0,0,119,177,177,177,177,177,98,56,0,0,0,0,0,102,254,220,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,169,254,137,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,169,254,57,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,169,254,57,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,169,255,94,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,169,254,96,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,169,254,153,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,169,255,153,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,96,254,153,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],\n",
    "                     [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]])\n",
    "\n",
    "#matrix dimensions\n",
    "raw_height= matrix.shape[0]\n",
    "raw_width=matrix.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc94db-2d86-4c87-abbc-dcb1cc724938",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Then visualise it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f6db7b-f593-4515-b709-21620d5f2da3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'matrix shape {matrix.shape}')\n",
    "fig,ax=plt.subplots(1,1,figsize=(4,4))\n",
    "ax.imshow(matrix,cmap='Greys_r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3472322e-00a9-4823-a343-e02c910d0726",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple version that does convolution explicitly and plots it out\n",
    "\n",
    "Demonstrate with a simple vertical edge detector that looks at the average of the diference between the pixels on the left and those to  the right of where the filter is on the raw image.\n",
    "\n",
    "Define a  3 x 3 kernel:\n",
    "\n",
    "| 1 | 0 | -1 |\n",
    "| 1 | 0 | -1 |\n",
    "| 1 | 0 | -1 |\n",
    "\n",
    "\n",
    "- Make a blank new image\n",
    "- Ignoring a 1 pixel wide border around the edges of the original image:\n",
    "- Centre a 3 x 3 mask over each pixel of original image in turn:  \n",
    "  - Call this the **subimage** \n",
    "  - multiply each pixel value in the subimage by the weight in the corresponding posiutwtion in the kernel\n",
    "  - add up result and put it into the position of the middle pixel **in the new image**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b695b1b2-62da-4aff-aab4-6fd8f0a1e4bf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#kernel dimensions\n",
    "kernel = np.array([[1,0,-1],\n",
    "                   [1,0,-1],\n",
    "                   [1,0,-1]])\n",
    "k_width= kernel.shape[1]\n",
    "k_height= kernel.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab70bf-317e-40ea-b318-dacc224a60eb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since we put the result of our calculations into the pixel in the middle, we lose a border around the edge of the raw image.\n",
    "- This is one pixel thick for a 3x3 kernel,\n",
    "- 2 kernels thick for a 5x5 kernel\n",
    "- and so on for rectangular kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c5c89-2207-4828-9d07-4923256daa41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lost_y = k_width//2 \n",
    "lost_x= k_height//2 \n",
    "\n",
    "new_width= raw_width- 2*lost_x\n",
    "new_height= raw_height-2*lost_y\n",
    "\n",
    "print(f'original image size {raw_width} by {raw_height}\\n'\n",
    "      f'kernel size {k_width} by {k_height}\\n'\n",
    "      f'so new image size is {new_width} by {new_height}\\n'\n",
    "      f'x and y lost borders are {lost_x} and {lost_y} pixels thick'\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f5c2fa-bf84-409a-919a-736ee8879574",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Start by making arrays to hold the subimage and new image we are creating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4925fdd-7415-465e-9b2f-f23413a56d19",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sub_image = np.zeros((k_width, k_height),dtype='uint8')\n",
    "new_image = np.zeros((new_height,new_width),dtype='uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "083f8496-3881-4a43-b6f7-25c3f412dc42",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Create three subplots to show our results on \n",
    "- left image is original  \n",
    "  with a yellow box showing where the kernel is at the moment\n",
    "- middle box shows the sub-image under the current position  \n",
    "  and in the title we show the calculated value \n",
    "- right hand box shows the transformed image as we build it up \n",
    "\n",
    "Notes:\n",
    "1. These boxes have different scales!  \n",
    "2. To use the matplotlib animate function we create handles 'a1,a2,a3' to the things that we want to be changed within each subimage as we do the convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d5a135-a76f-4746-bf6a-47acfa28546a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(nrows=1,ncols=3,figsize=(10,5))\n",
    "\n",
    "#show raw image\n",
    "ax[0].imshow(matrix,cmap='gray',vmin=0,vmax=255)\n",
    "#add yellow outline of kernel position\n",
    "a0= ax[0].add_patch(Rectangle([0,0 ], k_width, k_height, fill=False, edgecolor='yellow', lw=2))\n",
    "\n",
    "#middle shows the subimage lying under the current kernel position\n",
    "a1=ax[1].imshow(sub_image,cmap='gray',vmin=0,vmax=255)\n",
    "#we'll also show the corresponding kernel values in red\n",
    "for subx in range (k_width):\n",
    "    for suby in range (k_height):\n",
    "        text2= ax[1].text(suby, subx, kernel[subx, suby],\n",
    "                       ha=\"left\", va=\"bottom\", color=\"r\")\n",
    "        \n",
    "#right image shows the transformed image        \n",
    "        \n",
    "a2= ax[2].imshow(new_image,cmap='gray',vmin=0,vmax=255)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee90f799-a6b3-47a5-a371-3034f0a49bab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### code that does step-by step convolution\n",
    "- init() function needed by FuncAnimation()\n",
    "- then convolution  to get new value for one pixel and update the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd69285a-692e-49cf-ae9e-3b8bcc2e1556",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    ''' show original image to start the animation'''\n",
    "    a0.set(x=-1,y=-1)\n",
    "    a1.set_data(sub_image)\n",
    "    a2.set_data(new_image)\n",
    "    return a0,a1,a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc939e-5df3-443f-b9ec-1bdd6e920609",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def animate_convolution_step(i):\n",
    "    '''\n",
    "    Performs one step of convolution for each step(i)\n",
    "    - Translates i into pixel co-ordinates x,y\n",
    "    - Gets subimage centred on that pixel\n",
    "    - Multiplies by kernel\n",
    "    - Puts sum into  feature_map pixel x,y\n",
    "    And updates images\n",
    "    '''\n",
    "    #Step 1: get co-ords for next frame\n",
    "    x = i% new_width\n",
    "    y= i//new_width\n",
    "    \n",
    "\n",
    "    #Step 2: move  coloured box on the original image\n",
    "    a0.set(y=y-lost_y,x=x-lost_x)\n",
    "    ax[0].set_title(f'BB: {a0.get_xy()}')\n",
    "\n",
    "    # Step 3 extract sub_image - same size as kernel \n",
    "    sub_image=np.zeros((k_height,k_width),dtype=int)\n",
    "    # xpixel and ypixel are in original image co-ordinates\n",
    "    #loop over rows of kernel\n",
    "    for suby in range(k_height):\n",
    "        ypixel= (y- lost_y)  +suby\n",
    "        #and columns\n",
    "        for subx in range (k_width):\n",
    "            xpixel = (x -lost_x)  + subx\n",
    "            #copy raw iamge value into subimage\n",
    "            sub_image[suby][subx] = matrix[ypixel][xpixel]\n",
    "\n",
    "    #update middle display\n",
    "    a1.set_data(sub_image)\n",
    "\n",
    "    #Step 4 perform element-wise multiplication\n",
    "    # result is same size as kernel\n",
    "    result = kernel * sub_image\n",
    "    \n",
    "    #linear sum to get pixel value for new image\n",
    "    new_pixel_val = result.sum()\n",
    "    new_pixel_val= np.clip(new_pixel_val,0,255)\n",
    "    ax[1].set_title(f'computed value {new_pixel_val}')\n",
    "\n",
    "    #update,  then display new image\n",
    "    new_image[y][x]=int(new_pixel_val)\n",
    "    a2.set_data(new_image)\n",
    "    \n",
    "    #return the three transformed parts of the output image\n",
    "    return a0, a1, a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5728edb-7c2a-432b-bd02-e3b06e34ce59",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Now run the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b12437-b468-4265-b4d9-9d5e4693a305",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "new_image = np.zeros((new_height,new_width),dtype='uint8')\n",
    "animation.FuncAnimation(fig, animate_convolution_step, init_func=init, frames=new_height*new_width-1, interval=5, blit=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a00ac55-04a7-4912-88be-d42f5e7b3bfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Here's some other example kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29434757-e77d-4411-b242-858da7553652",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The two edge detectors that combine to make Prewitt's filter\n",
    "vertical_kernel = np.array ([[1,0,-1],\n",
    "                             [1,0,-1],\n",
    "                             [1,0,-1]],dtype=int\n",
    "                          )\n",
    "horizontal_kernel = np.array ([[1,1,1],\n",
    "                               [0,0,0],\n",
    "                               [-1,-1,-1]],dtype=int\n",
    "                          )\n",
    "# A smoothing or averaging filter\n",
    "mean_pool5 = np.ones((5,5))/25\n",
    "\n",
    "#An example of a filter that responds to a specific shape\n",
    "corner_kernel = -1*np.ones((5,5),dtype=int)\n",
    "corner_kernel += 2*np.array([[0,1,0,0,0],\n",
    "                             [0,1,0,0,0],\n",
    "                             [0,1,0,0,0],\n",
    "                             [0,1,1,1,1],\n",
    "                             [0,0,0,0,0]])\n",
    "\n",
    "kernels= (vertical_kernel, horizontal_kernel, mean_pool5,corner_kernel)\n",
    "k_names = ('vertical','horiz','mean pooling 5x5','corner')\n",
    "\n",
    "for i in range(len(kernels)):\n",
    "    print(f' {k_names[i]}\\n{kernels[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766a33d7-c744-468d-966e-5f4f1a5f0052",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fig,axs = plt.subplots(nrows=len(kernels),ncols=3, figsize=(6,12))\n",
    "results = []\n",
    "for i in range(len(kernels)):\n",
    "    #show original\n",
    "    axs[i][0].imshow(matrix,cmap='gray', vmin=0, vmax=255)\n",
    "    if(i==0):\n",
    "        axs[i][0].set_title('Original')\n",
    "    axs[i][0].tick_params(left=False, bottom=False)\n",
    "    \n",
    "    #show kernel\n",
    "    sns.heatmap(kernels[i], cbar=False, annot=True, annot_kws={\"size\": 8},square=True, cmap='Greys_r', vmin=-1, ax=axs[i][1])\n",
    "    axs[i][1].set_title(k_names[i])\n",
    "    \n",
    "    #transformed image\n",
    "    output = conv2d(matrix, kernels[i])\n",
    "    output=np.clip(output,0,255)\n",
    "    axs[i][2].imshow(output, cmap='gray', vmin=0, vmax=255)\n",
    "    results.append(output)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "601b2b56-9f66-4900-9f6f-0a9e5d6bbccc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The maths of how convolutional filters work\n",
    "\n",
    "### Preliminaries:\n",
    "For ease assume a 3 x 3 kernel *k*, and index cells from 1 to 9 (starting with the top row and going left to right)\n",
    "i.e.\n",
    "|  | | |\n",
    "|---|---|----|\n",
    "| 1 | 2 | 3 |\n",
    "| 4 | 5 | 6 |\n",
    "| 7 | 8 | 9 |\n",
    "\n",
    "and use *K* for the kernel and $s[i]$ for the sub-image centred on pixel $i$\n",
    "\n",
    "So at each point $i$ we calculate\n",
    "\n",
    "$ newimage[i] = k_1s[i]_1 +k_2s[i]_2 +k_3s[i]_3 + k_4s[i]_4 + k_5s[i]_5 + k_5s[i]_5 + k_6s[i]_6 + k_7s[i]_7 + k_8s[i]_8 + k_9s[i]_9$\n",
    "\n",
    "which for short (and to let us generalise to **n= height x width** sized kernels) we write as\n",
    "\n",
    "$newimage[i] = \\sum_{j=1}^n k_j s[i]_j $\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b11fcc-b5d9-4e2c-87a6-42c809e8a676",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dot Product\n",
    "But $d = \\sum_{j=1}^n k_j s[i]_j $this is exactly the form for the scalar (dot) product of two vectors $k$ and $s[i]$ in k-dimensional space\n",
    "\n",
    "![dot product of vectors](images/dotproduct.jpg)\n",
    "\n",
    "And since $-1 \\leq cos(\\theta) \\leq 1$, the value $d = |k| . |s[i]| . cos(\\theta)$ is going to be biggest when $cos(\\theta)=1$ which happens when the two vectors are pointing in exactly the same direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b113c11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**In other words, the output at a pixel is going to be biggest when the subimage is some constant (scalar) scalar multiple of the kernel**\n",
    "\n",
    "**So each filter can be thought of as a feature detector,**\n",
    "- and the output of convolving an image with a filter is sometimes called a *feature map*\n",
    "- where at each point the outpout corresponds to the quality of the match between the local subimage and the filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16932d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Terminology\n",
    "- A *scalar* is a single value with no dimension\n",
    "- A *vector* is a combination of $n \\geq2$ values that can be thought of as having a direction in *n-D* space\n",
    "- The *norm, |v|* of a vector *v* is its length  \n",
    "  - Typically we use $L_2$ norm, which is calculated as the square root of the sum of squared (hence $L_2$ values its components $ L_2(v) = \\sum_{i=1}^n v_i^2$\n",
    "  - This is the famous Pythagor;'s Theorm about the  *length of the Hypotenuse* you learned about at school"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dffcb67-fd6b-4495-a6e2-2e8dac99053d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Relationship to perceptrons\n",
    "\n",
    "You may have noticed that the equation above looks identical to how we calculate the input to a perceptron.\n",
    "\n",
    "The difference is that a perceptron is usually connected to all the inputs, or to nodes in the previous layer,\n",
    "- so it has many more weights,\n",
    "- even in our  hand-writtn number example the would be 28x28-784 weights to learn\n",
    "\n",
    "In contrast  in a Convolutional Neural Network, a node has fewer weights \n",
    " - just those for its 'receptive field' (9 in our case),\n",
    " - and the same few weights are applied repeatedly to different sub-regions of the inputs.\n",
    " \n",
    "This difference means we can have more nodes in a convoltional layer - or more layers,\n",
    "for the same amount of data to learn from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b946eed-0ddf-4db2-9112-6829775275e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# So that is how digital image processing used to work\n",
    "\n",
    "Different filters were hand-designed to  do different things\n",
    "- horizontal / vertical edge detection\n",
    "- smoothing (averaging)\n",
    "- texture detection\n",
    "\n",
    "Features can operate on color (three-channels): just add a third dimension to the kernel and apply it to the same subimage for each channel\n",
    "\n",
    "There is no reason why we need restrict ourselves to linear combinations like sums\n",
    "- could use max, range, variance, thresholding of result from multiplying subimage by kernel\n",
    "- lets us do things like thresholding, erosion, dilation, \n",
    "\n",
    "We can make combinations of filters, and feed the output from one filter into another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1970422c-866e-4b92-bc9b-4fcc884ecc4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Image Processing Experts made lots of money ! \n",
    "- designing transformations for specific applications\n",
    "- doing things like segmenting images into regions\n",
    "- so we could then apply ML based on the content\n",
    "\n",
    "### Was there a better way?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac2f34b-6199-42b7-bafe-f0a8aea14dab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Example: Using Interactive EAs to evolve filter combinations and weightings:\n",
    "O. Pauplin, P. Caleb-Solly, and J.E. Smith.  \n",
    "User-Centric Image Segmentation using an Interactive Parameter Adaptation Tool.   \n",
    "Pattern Recognition, 43(2):519–529, February 2010. \n",
    "DOI 10.1016/j.patcog.2009.03.007O. \n",
    "[pdf](https://doi.org/10.1016/j.patcog.2009.03.007)\n",
    "\n",
    "Describes work done on an EU-funded project with people like Sony, industrial part manufacturers. \n",
    "This is where my interest in interactive AI (EAs and ML) really kicked in:\n",
    "- non-experts may not be able to define image processing kernels\n",
    "- but they can certainly say which combinations are better/worse  \n",
    "  at highlighting things they are interested in!\n",
    "\n",
    "### This became part of a bigger hybrid system, but still got trained separately to the ML part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb5ccf8-e092-453e-a640-a6ed8e7f40d9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Convolutional Networks the key idea: End-to-End training\n",
    "\n",
    "### Nodes in first few layers of network use convolution\n",
    "- lots fewer weights to learn for each node\n",
    "- nodes often called filters\n",
    "\n",
    "### Layers of progressively more complex filters get applied to 'feature maps' created by earlier layers\n",
    "-  'max pooling' (kernel of ones with 'max' operator):  \n",
    "   effectively says *there was a feature detected somewhere in this kxk area*\n",
    "   \n",
    "### Finally *flatten* the n-dimensional feature maps into one long array\n",
    "- then apply fully connected nodes over the top to do classification/ regression\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
