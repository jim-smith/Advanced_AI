{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92d1575e-db9a-4452-8b62-45fe7ede2e98",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"width:400px, text-color:black\"> \n",
    "For this task we will use  the <a href=\"https://www.kaggle.com/datasets/nipunarora8/age-gender-and-ethnicity-face-data-csv?select=age_gender.csv\">age-gender-and-ethnicity-face dataset</a> which contains a number of images, each accompanied with features recording the age,  gender,  and ethnicity of the person in the image. This is a  version of the <a href=\"https://susanqq.github.io/UTKFace/\" > UTKface dataset</a>, modified to make it easer to load as one file.<br>\n",
    "Please note that\n",
    "    <ul>\n",
    "        <li>It happens not contain any people who self-identified as non-binary. So gender is labelled as 0 (male) or 1 (female)</li>\n",
    "        <li> Ethicity is coded as an integer from 0 to 4, denoting White, Black, Asian, Indian, and Others (like Hispanic, Latino, Middle Eastern) (terms from original site).</li>\n",
    "          <li>  Each row of this version of the dataset contains integer values for the three features, a string with the name or the original jpg from the UTK archive, and a feature called 'pixels' which contains the 48x48 pixels values as a string.</li>\n",
    "    </ul>\n",
    "\n",
    "The next two cells load the data into a pandas dataframe and then shpw the first ten lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c738466a-1e08-4fe7-9a63-31a2e0493e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import socket\n",
    "import random\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical # convert to one-hot-encoding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers as keras_layers\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc2b077-95be-4799-aaa1-97608f426e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (socket.gethostname()=='csctcloud'): #on csctcloud\n",
    "    path=\"/home/common/datasets/\"\n",
    "else: #machine specific- this is for jim's development\n",
    "    path = \"../datasets\"\n",
    "dataframe = pd.read_csv(path+'/utk/teacher_pupil.csv')\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11459d9e-8893-4d63-9a8d-d9b13418b2d6",
   "metadata": {},
   "source": [
    "### Let's start by splitting the images into appropriate numpy arrays\n",
    "- We first convert the 'pixekls' column of the dataframe into a numpy array\n",
    "- then split each row into a sub-array using a space as a seperator, \n",
    "- before reshaping our array from 23705x2304 floats into 23705 * (48x48) images\n",
    "- the conversion of the labels is first makes a straightforward 1d array, \n",
    "  then uses that to put 1s into the right column of a 2d array for one-hot encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0816f1-f700-475c-8d75-c3e27eb93d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = dataframe['pixels'].to_numpy()\n",
    "print(f'original shape of imgs array {imgs.shape}')\n",
    "imgs = np.array([x.split(' ') for x in imgs], dtype=float)\n",
    "print(f' shape  after splitting: {imgs.shape}')\n",
    "imgs = imgs.reshape(-1,48,48,1).astype(int)\n",
    "print(f'shape  after reshaping into 2d images with one channel: {imgs.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1d87d5-1d3f-4de6-ba85-4e7058cee687",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels= dataframe['gender'].to_numpy()\n",
    "y = to_categorical(labels,num_classes=2)\n",
    "print(f'shape of labels is {labels.shape} and y is {y.shape}')\n",
    "print(f'Split of males:females in the labels is {np.unique(labels,return_counts=True)[1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9711ef0-1203-4dc9-be42-a39a612b7de0",
   "metadata": {},
   "source": [
    "### This is what ten randomly chosen images look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62704178-6fa8-484b-b631-4910771db84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs=plt.subplots(2,5,figsize=(7.5,5))\n",
    "for i in range(10):\n",
    "    img = random.randint(0,labels.shape[0])\n",
    "    axs[i//5][i%5].imshow(imgs[img],cmap='gray')\n",
    "    axs[i//5][i%5].set_title(f'{ \"male\" if labels[img]==0 else \"female\"}\\n y[i]= {y[img]}')\n",
    "                                   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d697a7-b638-448f-a2cd-c227e5487b61",
   "metadata": {},
   "source": [
    "### Finally use standard sklearn function to split data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b82d3e-45ca-4037-8690-dbc67bd99628",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train,X_test,y_train,y_test= train_test_split(imgs,y,test_size=7705,shuffle=True,stratify=y)\n",
    "print(f'For sanity-checking: train and test arrays have shapes {X_train.shape}, {X_test.shape},{y_train.shape},{y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d416ddd5-80a9-4a7b-b71d-537c03141c97",
   "metadata": {},
   "source": [
    "## Now the convnet bit\n",
    "- start by specifying  a function to create a straightforward CNN using keras sequential model interface\n",
    "- then make a model and train it\n",
    "\n",
    "The architecture is inspired by [this kaggle post](https://www.kaggle.com/code/amishaasrani/gender-detection-by-cnn)\n",
    "It introduces some new types of layer into each convolution-maxpooling block, which implement some standard tricks to improve deep networks training.\n",
    "1. *Batch normalisation* is a method that attempts to reduce the random effects of dividing the data into batches.  \n",
    "   It works by scaling the outputs from each batch of data so they lie roughly within constant bounds estimated as the mean of the training data +/- the std. deviation of the training data.  \n",
    "   The net effect is usually to make it **faster to train** a network.  \n",
    "   [keras documentation here](https://keras.io/api/layers/normalization_layers/batch_normalization/)  \n",
    "   [Machine Learning Mastery blog here](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)\n",
    "2. *Dropout* is a **regularisation** technique applied to try and reduce the number of variables (non-zero weights) in the learned model.  \n",
    "   It works  by effectively pruning connections.  During training a fraction (0.2 in this case) of the nodes are arbitrarily 'switched off' for each batch,  so that the back-propagation can then reduce weight that do not seme to have any effect. \n",
    "   The net effect is usually **to help prevent over-fitting**. \n",
    "   [keras documentation here](https://keras.io/api/layers/regularization_layers/dropout/)  \n",
    "   [Machine Learning Mastery blog here](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce00f139-a031-4aa9-b950-52e97f3eaff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_model(num_classes):\n",
    "    model = Sequential()\n",
    "    \n",
    "    #first block of layers\n",
    "    model.add(keras_layers.Conv2D(32, kernel_size=(3, 3), activation='relu', padding = \"same\", input_shape=(48,48,1)))\n",
    "    model.add(keras_layers.BatchNormalization())\n",
    "    model.add(keras_layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(keras_layers.Dropout(0.2))\n",
    "    \n",
    "    #second block of layers\n",
    "    model.add(keras_layers.Conv2D(64, kernel_size=(3,3),activation=\"relu\",padding=\"same\"))\n",
    "    model.add(keras_layers.BatchNormalization())\n",
    "    model.add(keras_layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(keras_layers.Dropout(0.2))\n",
    "    \n",
    "    #third block of layers\n",
    "    model.add(keras_layers.Conv2D(64, kernel_size=(3,3),activation=\"relu\",padding=\"same\"))\n",
    "    model.add(keras_layers.BatchNormalization())\n",
    "    model.add(keras_layers.MaxPool2D(pool_size=(2,2)))\n",
    "    model.add(keras_layers.Dropout(0.2))\n",
    "    \n",
    "    #fully connected layers followed by softmax output\n",
    "    model.add(keras_layers.Flatten())\n",
    "    model.add(keras_layers.Dense(256,activation=\"relu\"))#256\n",
    "    model.add(keras_layers.Dense(num_classes, activation=\"softmax\"))\n",
    "    \n",
    "    model.compile(optimizer='Adam',\n",
    "              loss= 'BinaryCrossentropy',\n",
    "              metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9ea706-4ea9-47c8-9306-34a0dce953ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "convnet = conv_model(num_classes=2)\n",
    "convnet.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6eb8ae-4a3c-44b4-9b8a-bd876a8f3e57",
   "metadata": {},
   "source": [
    "### Train the model using an early stopping criteria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf483629-a87f-4eba-8a95-b9f80e90d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss',patience=10, \n",
    "                               min_delta=0.001,\n",
    "                               restore_best_weights=True)\n",
    "\n",
    "\n",
    "history= convnet.fit(X_train,y_train,validation_split=0.1,epochs=50,batch_size=64, callbacks=early_stopping,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4fe445-48c0-430c-9e51-4561095d1a72",
   "metadata": {},
   "source": [
    "## plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa3e069-1bf5-48c9-91d1-26aa418809c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.epoch, history.history[\"accuracy\"],history.history['val_accuracy'])\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.suptitle('Training History')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14562a31-6563-4852-a802-28551bd697d0",
   "metadata": {},
   "source": [
    "### show test accuracy and confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae72ba8-b090-4f5c-ba9d-e996eec042d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = convnet.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"Test loss: {}\".format(loss))\n",
    "print(f\"Test Accuracy: {acc*100}%\")\n",
    "ypred = convnet.predict(X_test,verbose=0)\n",
    "#convert ypred and y_test back to categorical labels\n",
    "y_true= np.argmax(y_test,axis=1)\n",
    "y_pred = np.argmax(ypred,axis=1)\n",
    "\n",
    "ConfusionMatrixDisplay.from_predictions(y_true,y_pred,display_labels=['male','female'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e1d092-9c4a-4d22-bae7-12311e28c35b",
   "metadata": {},
   "source": [
    "## Keras Support for data augmentation\n",
    "There is a list of preprocessing layers, including data augmentation, and different ways of using them in a workflow in [this Keras guide](https://keras.io/guides/preprocessing_layers/).\n",
    "\n",
    "For now we will begin by illustrating the effects of some common functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69a27bc-2d48-4b72-abea-525929ffb655",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = [Sequential(keras_layers.RandomFlip()),\n",
    "                     Sequential(keras_layers.RandomContrast(1)), \n",
    "                     Sequential(keras_layers.RandomRotation(0.1)),\n",
    "                     Sequential(keras_layers.RandomZoom(0.5))]\n",
    "names=['flip','contrast','rotation','zoom']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6209c5f4-2f71-4b6b-ad33-58f9bd0af62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.get_logger().setLevel('ERROR')\n",
    "fig,axs= plt.subplots(len(names),5,figsize=(10, 10))\n",
    "first_image = X_train[0]\n",
    "for row in range(len(names)):\n",
    "    axs[row][0].set_ylabel(names[row])\n",
    "    for col in range(5):\n",
    "        augmented_image = data_augmentation[row](\n",
    "        tf.expand_dims(first_image, 0), training=True\n",
    "        )\n",
    "        axs[row][col].imshow(augmented_image[0].numpy().astype(\"int32\"),cmap='gray')\n",
    "plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8118c7-e63a-444b-a83c-06c84cd225f7",
   "metadata": {},
   "source": [
    "# so now to create a pipeline that will be used in training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef5bf675-a3d8-403f-a449-de470032671c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gender_face_data(seed=12345):\n",
    "    if (socket.gethostname()=='csctcloud'): \n",
    "        path=\"/home/common/datasets/\"\n",
    "    else: #machine specific\n",
    "        path = \"../datasets\"\n",
    "    dataframe = pd.read_csv(path+'/utk/teacher_pupil.csv')\n",
    "    \n",
    "    imgs = dataframe['pixels'].to_numpy()\n",
    "    imgs = np.array([x.split(' ') for x in imgs], dtype=float)\n",
    "    imgs = imgs.reshape(-1,48,48,1).astype(int)\n",
    "    labels= dataframe['gender'].to_numpy()\n",
    "    y = to_categorical(labels,num_classes=2)\n",
    "    X_train,X_test,y_train,y_test= train_test_split(imgs,y,test_size=7705,shuffle=True,stratify=y,random_state=seed)\n",
    "    return X_train,X_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9543338d-5c53-4411-a189-c8e90db755f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a function that will let us choose which augemtnatinos to use\n",
    "def make_augmenter(flip=True,contrast=True,rotation=True,zoom=True):\n",
    "    augmenter = Sequential()\n",
    "    if flip:\n",
    "        augmenter.add(keras_layers.RandomFlip())\n",
    "    if contrast:\n",
    "        augmenter.add(keras_layers.RandomContrast(1))\n",
    "    if rotation:\n",
    "        augmenter.add(keras_layers.RandomRotation(0.1))\n",
    "    if zoom:\n",
    "        augmenter.add(keras_layers.RandomZoom(0.5))\n",
    "    return augmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aed861f-02e5-4891-b59b-916ea3422d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a tf.data pipeline of augmented images (and their labels)\n",
    "#this time we have to take out the validation set manually\n",
    "\n",
    "def get_augmented_data_streams(X_train,y_train,batchsize):\n",
    "    #this split ends up losing 32 faces ...\n",
    "    split=15000\n",
    "    valsize=1000\n",
    "    assert  split+valsize <=len(y_train),f\"can't split data we don't have {len(y_train)}\"\n",
    "    x_tr= X_train[:split,:]\n",
    "    y_tr=y_train[:split]\n",
    "    x_val= X_train[split:split+valsize,:]\n",
    "    y_val=y_train[split:split+valsize]\n",
    "    assert split%batchsize==0,\"training set size must be multpile of batchsize\"\n",
    "    assert valsize%batchsize==0,\"validation set size must be multpile of batchsize\"\n",
    "\n",
    "    train_dataset = tf.data.Dataset.from_tensor_slices((x_tr, y_tr))\n",
    "    train_dataset = train_dataset.batch(batchsize).map(lambda x, y: (data_augmentation(x), y))\n",
    "    validation_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(batchsize)\n",
    "    return train_dataset,validation_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2aa5fc-cd1d-4539-98b0-8eb3959b2e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now put all the pieces together into a function that can be called in a loop \n",
    "\n",
    "def run_experiment (flip=False,contrast=False,rotation=False,zoom=False,batchsize=50):\n",
    "    batchsize=batchsize\n",
    "    \n",
    "    X_train,X_test,y_train,y_test = get_gender_face_data()\n",
    "    data_augmentation= make_augmenter(flip=flip,contrast=contrast,rotation=rotation,zoom=zoom)\n",
    "    train_dataset,val_dataset = get_data_streams(X_train,y_train, batchsize)\n",
    "    print('Data made')\n",
    "    \n",
    "    augmented_cnn = conv_model(num_classes=2)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss',patience=10, \n",
    "                               min_delta=0.001,\n",
    "                               restore_best_weights=True)\n",
    "    history= augmented_cnn.fit(train_dataset,steps_per_epoch=split/batchsize,\n",
    "                               epochs=50,batch_size=batchsize,\n",
    "                              validation_data=validation_dataset, callbacks=early_stopping,\n",
    "                               verbose=True)\n",
    "    loss, acc = augmented_cnn.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\"Test loss: {}\".format(loss))\n",
    "    print(f\"Test Accuracy: {acc*100}%\")\n",
    "    ypred = augmented_cnn.predict(X_test,verbose=0)\n",
    "    #convert ypred and y_test back to categorical labels\n",
    "    y_true= np.argmax(y_test,axis=1)\n",
    "    y_pred = np.argmax(ypred,axis=1)\n",
    "\n",
    "    cm=ConfusionMatrixDisplay.from_predictions(y_true,y_pred,display_labels=['male','female']) \n",
    "    return cm.confusion_matrix\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fc8fd4-51b4-4158-bbb6-f7c50842db54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this runs an expereiment\n",
    "test_res= run_experiment(flip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edee27f-cf76-4e15-a0ba-d0891e32419b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95046ac3-2d88-42bf-91fc-a189404b0362",
   "metadata": {},
   "source": [
    "# Questions to investigate:\n",
    "1. Which of these are valid transformations for human faces?:\n",
    " - horizontal shifts\n",
    " - vertical shifts\n",
    " - rotation\n",
    " - horizontal flips\n",
    " - vertical flips\n",
    " \n",
    "Does it make a difference what the task is, i.e. gende recognition vs. recognising a specific person?\n",
    " \n",
    "To do this investigation using appropriate scientific method, treat each of these as a hypothesis to be tested. Take a number of observations (e.g. accuracy of trained model) for each case (e.g. using horizontal flips vs not using horizontal flips) then compare the mean results and use appropriate statistcal tests to determine whether the results are statistically significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66cd00-3f13-44fb-b9f2-233dd19ad0ec",
   "metadata": {},
   "source": [
    "# The main task:\n",
    "\n",
    "- Use the different pipeline components above to experiment with different sorts of data augmenation available within keras e.g. rotations, zoom,contrast changes,  and vertical/ horizontal flips. There are others available that only require a minor extension ot my make_augmenter() function.\n",
    "- Design an appropriate methodology to evaluate what difference they make singly or in combination to the classification accuracy of the trained system?  \n",
    "  *Hint*: If you are making several changes to a system you need some way of knowing which have had an effect: [ilustrated in 200 words](https://thaddeus-segura.com/data-aug/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44f3646-6c0d-49aa-9e28-6bfeb87c9572",
   "metadata": {},
   "source": [
    "### class discussion\n",
    "does data augmentation provide a away of addressing:\n",
    "- ethical concerns about under-representation of certain groups\n",
    "- safety concerns for example wrt autonomous vehicles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920111a8-07f4-4130-b585-fd44e86f6d42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py10",
   "language": "python",
   "name": "py10"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
