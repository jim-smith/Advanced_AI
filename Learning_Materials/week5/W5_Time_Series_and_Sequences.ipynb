{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66fc3981",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Week 5: Machine Learning for  Time Series and Sequences\n",
    "\n",
    "- What is a time-series / sequence problem \n",
    "\n",
    "- windowed approaches (1-D CNN)\n",
    "- recurrent neural networks (RNN)\n",
    "- Long Short Term Memory (LSTM)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a48737-433c-4502-a204-e82a771142dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Examples of a time series prediction problem\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/dimitreOliveira/MachineLearning/master/Kaggle/Store%20Item%20Demand%20Forecasting%20Challenge/time-series%20graph.png\" width=800 alt=\"example of demand changing over time\"></img>\n",
    "\n",
    "Demand forecasting problem from Kaggle [link to page](https://www.kaggle.com/code/dimitreoliveira/deep-learning-for-time-series-forecasting)\n",
    "\n",
    "Other examples: Air Quality Prediction, Stock Market Trading, Traffic Flows (cars/people/packet network), ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9157b6e-f29d-4314-b0f1-ab21ad0c7a6f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 'Traditional' statistical approaches: (S) ARIMA (X)\n",
    "\n",
    "<div style=\"float:right; width:600\">\n",
    "<img src = \"https://media.geeksforgeeks.org/wp-content/uploads/20200131170455/Screenshot-2020-01-31-at-5.04.16-PM.png\" width=600 ></img><br>\n",
    "Example from geeks for geeks. <a href=\"https://www.geeksforgeeks.org/python-arima-model-for-time-series-forecasting\">Page link</a>\n",
    "</div>\n",
    "\n",
    "- **S** = Seasonal Component  \n",
    "  repeating pattern that modifies signal\n",
    "- **AR** = Auto Regression  \n",
    "  use of previous values to predict next value\n",
    "- **I** = Integration:   \n",
    "  absolute values ->  differences between time-steps.  \n",
    "  Helps account for trends\n",
    "- **MA**= Moving Average:   \n",
    "  assumes error is related to error of previous terms.  \n",
    "  you can also think of it as a smoothing effect.\n",
    "- **X** = exogenous variables:   \n",
    "  e.g.for predicting NOx levels its useful to also take into account  \n",
    "  wind speed, temperature, humidity, traffic flows ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8019d260-e56a-4be0-b1df-b37ddb4031fa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Related Problem: Sequence prediction\n",
    "\n",
    "Typically (but not always) classification rather than regression.\n",
    "\n",
    "Forward (left to right)\n",
    "\n",
    "|Token 1|Token 2| Token 3|Token 4|Token 5| Token 6|\n",
    "|---|---|---|---|---|---|\n",
    "| The | name's | Bond |, | James| ? |\n",
    "  \n",
    "Bidirectional\n",
    "\n",
    "|Token 1|Token 2| Token 3|Token 4|Token 5|Token 6|\n",
    "|---|---|---|---|---|---|\n",
    "| The | name's | ? |, | James| Bond |\n",
    "  \n",
    " Lots more on this (language modelling) in the next few weeks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918c8288-0f72-4951-bcbd-093fef23f84b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's the difference between classification and regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cf746a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Loss functions used to drive training  \n",
    "  Mean Squared Error/ Mean Absolute Error vs. cross entropy  \n",
    "  MSE still allows for Maximum Likelihood Estimator analysis\n",
    "- Activation in final layer if using neural networks.   \n",
    "  linear rather than softmax/ logistic  \n",
    "- Choice of Performance metrics for model comparison and selection  \n",
    "  - Classification: ROC curves, confusion matrices\n",
    "  - Regression: possibly errors for different signal values\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925347ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Machine Learning Approaches to time series problems\n",
    "\n",
    "1 Treat as standard supervised learning problem but give the system a _window_ of n previous timesteps.  \n",
    "<div style=\"float:right; width:600\"><img src=\"simple-window-dataset.jpg\"/></div>\n",
    "\n",
    "- how do you fill in gaps?  \n",
    "  lose data or _impute_  :  \n",
    "  a bit like padding images in convolution\n",
    "- Easy to make in e.g. pandas\n",
    "- Easy to incorporate 'exogenous variables'.   \n",
    "  (other features) alongside the one to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef9883b-01ed-4b48-b6dc-b543702a876d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "2. Can use any 'standard' supervised ML. algorithm\n",
    " - especially 1-D CNN \n",
    "   - since time can be regarded as another dimension\n",
    "   - and we know CNN's cope with dimension well\n",
    " - quite a few recent papers suggesting CNNs work better than rrecurrent networks when:\n",
    "   - you only need to take into acount recent observations\n",
    "   - or a few  patterns regularly recur so can be learned by convolutional filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97b425",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### for example, Keras 1DConv Layer\n",
    "\n",
    "The inputs are 128-length vectors with 10 timesteps,  \n",
    "and the batch size is 4.\n",
    "```\n",
    ">>> input_shape = (4, 10, 128)\n",
    ">>> x = tf.random.normal(input_shape)\n",
    ">>> y = tf.keras.layers.Conv1D(\n",
    ">>>                     32, 3, \n",
    ">>>                     activation='relu',\n",
    ">>>                     input_shape=input_shape[1:]) (x)\n",
    ">>> print(y.shape)\n",
    "    (4, 8, 32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487718b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Methodological Differences:\n",
    "\n",
    "1. Does not make sense to use a randomised train/validate/ test split.  \n",
    "   So need strategy for dealing with trends via:  \n",
    "   pre-processing data and post-processing predictions\n",
    "\n",
    "2. N-fold cross validation still not a bad idea **but**\n",
    "   Ideally you would justify by a prior analysis of:  \n",
    "   long term trends,  \n",
    "   seasonal variations,  \n",
    "   frequency of extreme events\n",
    "   \n",
    "3. What 'naive' models to compare with?  \n",
    "   classifier: (probabilistically) predict most frequent class.  \n",
    "   regression: constant prediction **or**    \n",
    "   'predict-last-value'  -   can often give quite low MSE.  \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba026d57",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problems with windowed approach?\n",
    "\n",
    "> The name's Bond, James said when asked the author of the Paddington books\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe610cd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's not always easy to know how many time-steps the window should include.\n",
    "- 24 hours ?\n",
    "- 3 days? (lets you take account of weekends)\n",
    "- 7 days - lets you take account of things that happen one day a week.\n",
    "- ...\n",
    "\n",
    "Also, impractical to have super-long windows\n",
    "- because it means lots of parameters to learn  \n",
    "  ==> bigger optimistion problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221bcf1a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The answer (maybe)? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6905ef",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<div style=\"float:centre; color:red;\"><h3>Memory</h3></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41d03ed",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks (RNN)\n",
    "- Neural Network for data with a temporal relationship \n",
    "  i.e. sequences of data where the past (or future) data might influence the present.\n",
    "- RNN share an internal ‘hidden-state’ across all inputs.   \n",
    "  Effectively a network with memory of what it has previously seen.  \n",
    "  **This is what changes over time**\n",
    "- The key is that the weights and activations functions are the same for each time step\n",
    "\n",
    "![image.png](rnn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b545f2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Simple RNNs (Elman 1991)\n",
    "Nodes have a hidden state vector **h**\n",
    "\n",
    "Perceptron with input weight matrix _W_ and input vector _x_:  \n",
    "output update $$ y = logistic( W(x) )$$\n",
    "\n",
    "Elman \n",
    "Memory Node: \n",
    "$$ h_{t} = logistic( W_H\\begin{pmatrix} x \\\\ h_{t-1} \\end{pmatrix} ) $$\n",
    "     \n",
    "$$ y_t = logistic( W_y h_t ) $$\n",
    "\n",
    "Elman, J. L. (1991). _Distributed representations, simple recurrent networks, and grammatical structure._ Machine Learning, 7:195–225."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf569a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modern Version\n",
    "![Figure from Deep Learning](bengio-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f7cb1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Modern Version 2\n",
    "![Figure from Deep Learning, Goodfellow, Bengio and Courville](bengio-2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f60406b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Learning weights _U, V, W_ \n",
    "The key: \n",
    "- use stochastic gradient descent  \n",
    "  same as for Multi-Layer Perceptrons\n",
    "-  easier to think about for 'unrolled' version of RNN\n",
    "- **Back-Propagation Through Time**  \n",
    "  Gradient at 𝐿(𝑡): (total loss is sum of those at different time steps)\n",
    "- memory requirements become rapidly bigger  \n",
    "  because you need to store activations/signals at each time-step.\n",
    "  \n",
    "Problems?\n",
    " - vanishing and exploding gradients\n",
    " - so hard to remember things for long periods of time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e08e49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Long-Short Term Memory Nodes\n",
    "- Add 'gates', controlled by learnable weights that control how much the hidden state forgets or remembers changes in response to curent inputs\n",
    "\n",
    ">A common LSTM unit is composed of a cell, an input gate, an output gate[14] and a forget gate.[15] \n",
    "\n",
    ">The cell remembers values over arbitrary time intervals and the three gates regulate the flow of information into and out of the cell. \n",
    "\n",
    ">Forget gates decide what information to discard from a previous state by assigning a previous state, compared to a current input, a value between 0 and 1. A (rounded) value of 1 means to keep the information, and a value of 0 means to discard it. \n",
    "\n",
    ">Input gates decide which pieces of new information to store in the current state, using the same system as forget gates. \n",
    "\n",
    ">Output gates control which pieces of information in the current state to output by assigning a value from 0 to 1 to the information, considering the previous and current states\n",
    "\n",
    "(quotes from wikipedia)\n",
    "\n",
    "\n",
    "Hochreiter, S. and Schmidhuber, J. (1997). _Long short-term memory._ Neural Computation, 9(8):1735–1780."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5faad1f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Block diagram of LSTM](https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Peephole_Long_Short-Term_Memory.svg/2000px-Peephole_Long_Short-Term_Memory.svg.png)\n",
    "\n",
    "circle with x in = multiply components of signal by a scalar  \n",
    "Activation functions usually tanh.   \n",
    "$h_t$ is output.  \n",
    "**Weights at each gate input!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291509ab",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Example from [keras](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n",
    "````\n",
    ">> inputs = tf.random.normal([32, 10, 8])\n",
    ">> lstm = tf.keras.layers.LSTM(4)\n",
    ">> output = lstm(inputs)\n",
    ">> print(output.shape)\n",
    ">> (32, 4)\n",
    "````\n",
    "If you want to have more than one layer, (or do sequence to sequenbce) you need specify returning the 'unrolled' sequences\n",
    "````\n",
    ">> lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)\n",
    ">> whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)\n",
    ">> print(whole_seq_output.shape)\n",
    ">> (32,10,4)\n",
    ">> print(final_memory_state.shape)\n",
    ">> (32,4)\n",
    ">> print(final_carry_state.shape)\n",
    ">>  (32,4)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00d4de9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Impact of LSTMs\n",
    "Huge Leap forward for Deep Learning applied to:\n",
    "- time-series problems\n",
    "  - especially with long-term dependencies to learn\n",
    "- sequence to label\n",
    "  - text classfication/ sentiment analysis/ intent recognition/ ...\n",
    "- Can be used in a wide single layer or can stack layers\n",
    "  - either way, usually have a 'dense' layer (or two) afterwards (as for CNNs)  \n",
    "  \n",
    "- sequence to sequence \n",
    "  - translation between languages, ...\n",
    "  - typically use an encoder-decoder architecture\n",
    "  \n",
    "More on architectures next week.\n",
    "  \n",
    "Many variants: Gated Recurrent Units (GRU) ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1968b56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "MLenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
